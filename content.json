[{"title":"David Silver 强化学习 4 模型无关预测 Model-Free Prediction","date":"2017-10-15T13:01:30.000Z","path":"2017/10/15/David_Silver_RL_4/","text":"简介 Introduction上节课中通过动态规划能够解决已知 environment 的 MDP 问题，也就是已知 $S,A,P,R,\\gamma$，其中根据是否已知 policy 将问题又划分成了 prediction 和 control 问题，本质上来说这种 known MDP 问题已知 environment 即转移矩阵与 reward 函数，但是很多问题中 environment 是未知的，不清楚做出了某个 action 之后会变到哪一个 state 也不知道这个 action 好还是不好，也就是说不清楚 environment 体现的 model 是什么，在这种情况下需要解决的 prediction 和 control 问题就是Model-free prediction和Model-free control。显然这种新的问题只能从与 environment 的交互得到的 experience 中获取信息。 这节课要解决的问题是Model-free prediction，即未知environment的Policy evaluation，在给定的 policy 下，每个state的 value function 是多少。 蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning蒙特卡洛强化学习是假设每个 state 的 value function 取值等于多个 episodes 的 return Gt 的平均值，它需要每个 episode 是完整的流程，即一定要执行到终止状态。 蒙特卡洛策略评估 Monte-Carlo Policy Evaluation目标： 在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。 数学描述如下： 基于特定策略 $\\pi$ 的一个 Episode信 息可以表示为如下的一个序列：$S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, …, S_{t}, A_{t}, R_{t+1}, …, S_{k}$ ~ $\\pi$ t 时刻状态 $S_{t}$ 的收获：$G_{t} = R_{t+1} + \\gamma R_{t+2} + … + \\gamma^{T-1} R_{T}$其中 T 为终止时刻。 该策略下某一状态 s 的价值：$v_{\\pi}(s) = E_{\\pi} [ G_{t} | S_{t} = s ]$ 在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法： 首次访问蒙特卡洛策略评估在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，仅当该状态第一次出现时列入计算: 状态出现的次数加1： $N(s) \\leftarrow N(s) + 1$总的收获值更新： $S(s) \\leftarrow S(s) + G_{t}$状态s的价值： $V(s) = S(s) / N(s)$当 $N(s) \\rightarrow \\infty$ 时， $V(s) \\rightarrow v_{\\pi}(s)$ 每次访问蒙特卡洛策略评估在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，状态s每次出现在状态转移链时，计算的具体公式与上面的一样，但具体意义不一样。 状态出现的次数加1： $N(s) \\leftarrow N(s) + 1$总的收获值更新： $S(s) \\leftarrow S(s) + G_{t}$状态s的价值： $V(s) = S(s) / N(s)$当 $N(s) \\rightarrow \\infty$ 时， $V(s) \\rightarrow v_{\\pi}(s)$ 累进更新平均值 Incremental Mean这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。 理论公式如下： 这个公式比较简单。把这个方法应用于蒙特卡洛策略评估，就得到下面的蒙特卡洛累进更新。 蒙特卡洛累进更新对于一系列 Episodes 中的每一个： $S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, …, S_{t}, A_{t}, R_{t+1}, …, S_{k} $ 对于Episode里的每一个状态 $S_{t}$ ，有一个收获 $G_{t}$ ，每碰到一次 $S_{t}$ ,使用下式计算状态的平均价值 $V(S_{t})$ ：其中： 在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的 Episode 信息。此时可以引入参数 $\\alpha$ 来更新状态价值： 以上就是蒙特卡洛学习方法的主要思想和描述，由于蒙特卡洛学习方法有许多缺点（后文会细说），因此实际应用并不多。接下来介绍实际常用的TD学习方法。 时序差分学习 Temporal-Difference Learning时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习不完整的 Episode ，通过自身的引导（bootstrapping），猜测 Episode 的结果，同时持续更新这个猜测。 我们已经学过，在Monte-Carlo学习中，使用实际的收获（return） $G_{t} $来更新价值（Value）：$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha (G_{t} - V(S_{t}))$ 在 TD 学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$ 与下一状态 $S_{t+1}$ 的预估状态价值乘以衰减系数 $\\gamma$ 组成，这符合 Bellman 方程的描述：$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t}))$ 式中：$R_{t+1} + \\gamma V(S_{t+1})$ 称为 TD 目标值$\\delta_{t} = R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})$ 称为 TD 误差 BootStrapping 指的就是TD目标值 $R_{t+1} + \\gamma V(S_{t+1})$ 代替收获 $G_t$ 的过程，暂时把它翻译成“引导”。 示例——驾车返家 想象一下你下班后开车回家，需要预估整个行程花费的时间。假如一个人在驾车回家的路上突然碰到险情：对面迎来一辆车感觉要和你相撞，严重的话他可能面临死亡威胁，但是最后双方都采取了措施没有实际发生碰撞。如果使用蒙特卡洛学习，路上发生的这一险情可能引发的负向奖励不会被考虑进去，不会影响总的预测耗时；但是在TD学习时，碰到这样的险情，这个人会立即更新这个状态的价值，随后会发现这比之前的状态要糟糕，会立即考虑决策降低速度赢得时间，也就是说你不必像蒙特卡洛学习那样直到他死亡后才更新状态价值，那种情况下也无法更新状态价值。 TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。 基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来更新价值函数（各个状态的价值）。这里使用的是从某个状态预估的到家还需耗时来间接反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；而对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。 Monte-Carlo VS. Temporal Difference在谈两种算法的优劣前，先谈谈 Bias/Variance tradeoff 的问题。平衡 Bias/Variance 是机器学习比较经典的一个问题，bias 是指预测结果与真实结果的差值，variance 是指训练集每次预测结果之间的差值，bias 过大会导致欠拟合它衡量了模型是否准确，variance 过大会导致过拟合衡量了模型是否稳定。如果 $G_t$ 和 $R_{t+1} + \\gamma v_{\\pi} (S_{t+1})$ 跟真实值一样，那么就是无偏差估计。因为在MC算法中，它是将最终获得的 reward 返回到了前面的状态，因此是真实值，但是它采样的 episode 并不能代表所有的情况，所以会导致比较大的 variance 。而 TD的 $R_{t+1} + \\gamma v_{\\pi} (S_{t+1})$ 跟真实值是有偏差的，在计算的过程基于随机的状态、转移概率、reward 等等，涵盖了一些随机的采样，因此 variance 比较小。 示例——AB已知：现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。 问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即V(A)=？， V(B)=？ 答案：V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。 解释：应用MC算法，由于需要完整的 Episode ,因此仅 Episode1 可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。应用 TD 算法时，TD 算法试图利用现有的 Episode 经验构建一个 MDP（如下图），由于存在一个 Episode 使得状态A有后继状态 B ，因此状态A的价值是通过状态B的价值来计算的，同时经验表明 A 到 B 的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于 B 的状态价值。 确定性等价 Certainty EquivalenceMC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案，这一均方差用公式表示为： 式中，k 表示的是 Episode 序号， K 为总的 Episode 数量， t 为一个 Episode 内状态序号（第1,2,3…个状态等）， $T_{k}$ 表示的是第 k 个Episode总的状态数， $G^{k}_{t}$ 表示第 k 个 Episode 里 t 时刻状态 $S_{t}$ 获得的最终收获， $V(S^{k}_{t})$ 表示的是第 k 个 Episode 里算法估计的 t 时刻状态 $S_{t}$ 的价值。 TD算法则收敛至一个根据已有经验构建的最大可能的马儿可夫模型的状态价值，也就是说TD算法将首先根据已有经验估计状态间的转移概率： 同时估计某一个状态的即时奖励： 最后计算该MDP的状态函数。 三种强化学习算法Monte-Carlo, Temporal-Difference 和 Dynamic Programming 都是计算状态价值的一种方法，区别在于，前两种是在不知道Model的情况下的常用方法，这其中又以MC方法需要一个完整的Episode来更新状态价值，TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态S’及其转移概率以及对应的即时奖励来计算这个状态S的价值。 关于是否采用 Bootstrap： MC 没有引导数据，只使用实际收获；DP和TD都有引导数据。 关于是否用采样 Sampling: MC和TD都是应用样本来估计实际的价值函数；而DP则是利用模型直接计算得到实际价值函数，没有样本或采样之说。 下面的几张图直观地体现了三种算法的区别： MC: 采样，一次完整经历，用实际收获更新状态预估价值 TD：采样，经历可不完整，用喜爱状态的预估状态价值预估收获再更新预估价值 DP：没有采样，根据完整模型，依靠预估数据更新状态价值 上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。 当使用单个采样，同时不走完整个Episode就是TD； 当使用单个采样但走完整个Episode就是MC； 当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP； 当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。 需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。 TD(λ)TD(0) 是指在某个状态 s 下执行某个动作后转移到下一个状态 s′ 时，估计 s′ 的 return 再更新 s ，假如 s 之后执行两次动作转移到 s″ 时再反回来更新s的值函数，那么就是另一种形式，从而根据 step 的长度 n 可以扩展 TD 到不同的形式，当 step 长度到达当前 episode 终点时就变成了 MC。 定义 n-step 收获：$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + … + \\gamma ^{n-1}R_{t+n}+\\gamma ^nV(S_{t+n})$ n = 1 时 即 TD(0) 那么，n-step TD 学习状态价值函数的更新公式为：$V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^{(n)} - V(S_t))$ 既然存在 n-step 预测，那么n=？时效果最好呢? 前向认知 TD(λ) The Forward View of TD(λ)如果将不同的 n 对应的 $G_t^{(n)}$ 平均一下，这样能够获得更加 robust 的结果，而为了有效的将不同 $G_t^{(n)}$ 结合起来，对每个 n 的 $G_t^{(n)}$ 都赋了一个权重 $1-\\lambda,(1-\\lambda)\\lambda,…,(1-\\lambda)\\lambda^{n-1},\\lambda^{T-t-1}$ ，T 是状态的总数, t 表示了在第几个状态 ，所有的权重加起来为1,这样又能得到一组更新 value function 的公式。 $G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty }\\lambda^{n-1}G_t^{(n)}$为了更好的把episode的terminal state体现出来，我们可以写成下式：$G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{T-t-1 }\\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1}G_t^{(T-t)}$ $V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^{\\lambda} - V(S_t))$ 下图是各步收获的权重分配图 对于每个访问到的state，我们都是从它开始向前看所有的未来reward，并决定如何结合这些reward来更新当前的state。每次我们更新完当前state，我们就到下一个state，永不再回头关心前面的state。这种感觉就像下图一样： TD(λ)对于权重分配的图解 对于 n=3 的 3-步收获，赋予其在 $\\lambda$ 收获中的权重如左侧阴影部分面积，对于终止状态的 T-步 收获，T以后的所有阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。 在实际任务中这类算法却很少用，因为不便于实现：在n-step TD中，你需要等待 n 步之后观测得到的reward和state。如果n很大，这个等待过程就是个问题，存在很大的滞后性，这是一个待解决问题。那么n-step TD有什么意义呢？n-step TD的思想可以让我们更好理解接下来的”资格迹“方法。 后向认知 TD(λ) The Backward View of TD(λ)TD(λ)的后向视角非常有意义，因为它在概念上和计算上都是可行而且简单的。具体来说，前向视角只提供了一个非常好但却无法直接实现的思路，因为它在每一个timestep都需要用到很多步之后的信息，这在工程上很不高效。而后向视角恰恰解决了这个问题，采用一种带有明确因果性的递增机制来实现TD(λ)，最终的效果是在on-line case和前向视角近似，在off-line case和前向视角精确一致。 后向视角的实现过程中，引入了一个额外的和每个state都相关的存储变量，它的名字就叫做“资格迹”。在t时刻的状态s对应的资格迹，标记为 $Z_t(s) \\in \\mathbb{R}^+$ 。资格迹的更新方式如下： 稍微解释一下上面式子的含义：在一开始，每个episode中的所有状态都有一个初始资格迹，然后时间开始走。到下一个timestep，被访问到的那个状态，其资格迹为前一个时刻该状态资格迹乘上一个（decay param 乘 discount param），然后加1，表示当前时刻，该状态的资格迹变大；其他未被访问的状态，其资格迹都只是在原有基础上乘以（decay param*discount param），不用加1,表明它们的资格迹都退化了。 我们把λ成为“迹退化参数”，把上式这种更新方式的资格迹叫做“累加型资格迹”。因为它在状态被访问的时候累加，不被访问的时候退化。如下图： 常规的TD误差如下式：$\\delta_t = R_{t+1} + \\gamma V_t(S_{t+1})-V_t(S_t)$ 但是在TD(λ)的后向视角中，这个误差却可以影响所有“最近”访问过的状态的，对于这些状态来说，TD误差如下： $\\Delta V_t(s) = \\alpha \\delta_tZ_t(s) \\;\\;for \\; all \\; s\\in S$ 可以这么说：后向视角的TD(λ)，才是真正的TD(λ)。我们来更形象地表述一下后向视角的过程：每次在当前访问的状态得到一个误差量的时候，这个误差量都会根据之前每个状态的资格迹来分配当前误差。这就像是一个小人，拿着当前的误差，然后对准前面的状态们按比例扔回去，就像下图一样： λ=0:TD(λ)退化成TD(0)，除了当前状态，其他所有状态的资格迹为0。这时候，小人不再向前面扔误差量，而是只给当前状态了。随着λ变大，但是小于1,更多状态的误差被当前误差所影响，越远的，影响越小。我们称越远的状态被分配的credit（可以理解为信用）越少。 λ=1:当 $\\gamma$ 不等于1时， $\\delta_t$ 依照 $\\gamma^k$ 逐步递减；如果 $\\gamma$ 等于1，那么资格迹就不再递减。我们把λ=1时的TD算法也叫做TD(1)。 传统MC如果不走到episode结束，是什么都学不到的。而且传统MC只适用于非连续式，有明确终点状态的任务。再看TD(1)，读者可以把它想象成拖着一条移动的长尾巴的MC类算法（你可以把传统MC看成是有一条固定的长尾巴算法），可以即时地从正在进行的episode中学习，基于它的控制方法可以及时地修正后续episode的生成方式。因此TD(1)不仅适用于片段式任务，也适用于连续性任务。 Equivalence of Forward and Backward Views参考：https://zhuanlan.zhihu.com/p/27773020 小结下表给出了λ取各种值时，不同算法在不同情况下的关系。 内容的图标总结：","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 3 动态规划解决MDP的Planning问题","date":"2017-09-21T13:01:30.000Z","path":"2017/09/21/David_Silver_RL_3/","text":"简介 Introduction当问题具有下列特性时，通常可以考虑使用动态规划来求解： 第一个特性是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解； 子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。 马尔可夫决策过程（MDP）具有上述两个属性：Bellman方程把问题递归为求解子问题，价值函数就相当于存储了一些子问题的解，可以复用。因此可以使用动态规划来求解MDP。 我们用动态规划算法来求解一类称为“规划 Planning”的问题。“规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用规划来解决 Predict 和 Control 问题。 策略迭代 Policy Iteration这个解决途径主要分为两步： Policy Evaluation:基于当前的Policy计算出每个状态的value function $V$ （迭代计算直到收敛）\b Policy Improvment:基于当前的value function，采用贪心算法来找到当前最优的Policy $\\pi$ 如此反复多次，最终得到最优策略 $\\pi^{star}$ 和最优状态价值函数 $V^{star} $ ( star 代表 \b* )。 下图是一个叫Small Gridworld的例子，左上角和右下角是终点，γ=1，移动一步 reward=-1，起始的random policy是朝每个能走的方向概率相同。 策略改善 Policy Improvment 理论证明思考：很多时候，策略的更新较早就收敛至最优策略，而状态价值的收敛要慢很多，是否有必要一定要迭代计算直到状态价值得到收敛呢？ 值迭代 Value Iteration优化原则 Principle of Optimality一个最优策略可以被分解为两部分：从状态 s 到下一个状态 s’ 采取了最优行为 $A_*$ ；在状态 s’ 时遵循一个最优策略。 从上面原理出发，如果已知子问题的最优值 v∗(s′)，那么就能通过第一个Bellman Optimality Equation将 v∗(s) 也推出来。因此从终点开始向起点推就能把全部状态最优值推出来。Value Iteration 通过迭代的方法，通过这一步的 $v_k (s’)$ 更新下一步的 $v_{k+1}(s)$ ，最终收敛到最优的 v∗ ，需要注意的是中间生成的value function的值不对应着任何 policy。 考虑下面这个Shortest Path例子，左上角是终点，要求的是剩下每一个格子距离终点的最短距离，每走一步，reward=-1 因此，针对MDP要解决的两个问题，有如下几种方式来解决。针对prediction，因为它的目标是在已知的Policy下得到收敛的value function，因此针对问题不断迭代计算Bellman Expectation Equation就够了，但是control则需要同时获得最优的policy，那么在Iterative Policy Evaluation的基础上加入一个选择Policy的过程就行了，也就是上面的Policy Iteration，另外Value Iteration虽然在迭代的过程中没有显式计算出policy，但是在得到最优的value function之后就能推导出最优的policy，因此也能用做解决control问题。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"学徒学习 Apprenticeship learning via inverse reinforcement learning","date":"2017-09-15T13:01:30.000Z","path":"2017/09/15/Apprenticeship_learing/","text":"学徒学习是Ng（吴恩达）和Abbeel提出来的。学徒学习是这样：Agent从专家示例中学到回报函数，使得在该回报函数下所得到的最优策略在专家示例策略附近。 回报函数$R(s)$ 假设为：$R\\left(s\\right)=w^T\\cdot\\phi\\left(s\\right)$，其中$\\phi(s)$为映射特征的基函数，可以为多项式基底，也可以为傅里叶基底。文中是以线性函数为基底。逆向强化学习求的就是回报函数中的系数w。策略 $\\pi$ 的值函数为：$v_{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t R(s_t)]$将回报函数代入：$v_{\\pi}(s) = w^T E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$ 将上式右半部分定义为特征期望：$\\mu\\left(\\pi\\right)=E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$。需要注意的是，特征期望跟策略 $\\pi$ 有关，策略不同时，策略期望也不相同 当给定m条专家轨迹后，根据定义我们可以估计专家策略的特征期望为：$\\hat{\\mu}_E=\\frac{1}{m}\\Sigma_{i=1}^{m}\\Sigma_{t=0}^{\\infty}\\gamma^t\\phi\\left(s_{t}^{\\left(i\\right)}\\right)$其中，专家状态序列为专家轨迹:${ s_{0}^{(i)},s_{1}^{(i)},\\cdots }_{i=1}^{m}$ 逆向强化学习可以归结为如下问题： 找到一个策略，使得该策略的表现与专家策略相近。我们可以利用特征期望来表示一个策略的好坏，找到一个策略，使其表现与专家策略相近，其实就是找到一个策略 $\\tilde{\\pi}$ 的特征期望与专家策略的特征期望相近，即使如下不等式成立：$\\lVert\\mu\\left(\\tilde{\\pi}\\right)-\\mu_E\\rVert_2\\le\\epsilon$ 当该不等式成立时，对于任意的权重$\\lVert w\\rVert_1\\le 1$，值函数满足如下不等式： 算法 其中第二行的目标函数为: $t^{(i)}=\\max_{w:\\lVert w\\rVert_2\\le 1}\\min_{j\\in{0\\cdots(i-1)}}w^T(\\mu_E-\\mu^{(j)})$ 写成标准的优化形式为：注意，在进行第二行求解时，$\\mu^{\\left(j\\right)}$中的 $j\\in{0,1,\\cdots ,i-1}$是前i-1次迭代得到的最优策略。也就是说第i次求解参数时，i-1次迭代的策略是已知的。这时候的最优函数值t相当于专家策略 $\\mu_E$ 与i-1个迭代策略之间的最大边际。 我们可以从支持向量机的角度去理解。专家策略为一类，其他策略为另一类，参数的求解其实就是找一条超曲面将专家策略和其他策略区分开来。这个超平面使得两类之间的边际最大。 第四行是在第二行求出参数后，便有了回报函数$R=\\left(w^{\\left(i\\right)}\\right)^T\\phi$，利用该回报函数进行强化学习，从而得到该回报函数下的最优策略 $\\pi^{\\left(i\\right)}$。 总结学徒逆向强化学习方法分为两步:第一步在已经迭代得到的最优策略中，利用最大边际方法求出当前的回报函数的参数值；（该计算需要用到QP(二次规划)求解器或者\bSVM求解器。文中也给出了一种不\b使用\b求解器的简单算法。）第二步利用求出的回报函数的参数值进行正向强化学习方法求得当前最优的策略，然后重复第一步。 需要注意的是，$\\phi(s)$ 中输入的 s \b为 i 个特征：$s^1,s^2,…,s^i$，if 第 i 个特征存在，$s^i = 1$，else $s^i = 0$。又因为$\\lVert w\\rVert_1\\le 1$，所以$R\\le 1$。 ref: Abbeel, P., &amp; Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. International Conference on Machine Learning(Vol.11, pp.1). ACM.","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 2 MDP","date":"2017-08-18T13:01:30.000Z","path":"2017/08/18/David_Silver_RL_2/","text":"在强化学习中，马尔可夫决策过程（Markov decision process, MDP）是对完全可观测的环境进行描述的，也就是说观测到的状态内容完整地决定了决策的需要的特征。几乎所有的强化学习问题都可以转化为MDP。本讲是理解强化学习问题的理论基础。 马尔可夫过程 Markov Process马尔可夫性 Markov Property 某一状态信息包含了所有相关的历史 只要当前状态可知，历史信息 history 就可以被丢弃 当前状态就可以决定未来 The future is independent of the past given the present即该状态具有马尔可夫性。 可以用下面的状态转移概率公式来描述马尔可夫性：$P_{ss’} = P[S_{t+1}=s’|S_t=s]$ 下面的状态转移矩阵定义了所有状态的转移概率：式中n为状态数量，矩阵中每一行元素之和为1. 马尔可夫过程 Markov Process马尔可夫过程 又叫马尔可夫链(Markov Chain)，它是一个无记忆的随机过程，可以用一个元组表示，其中S是有限数量的状态集，P是状态转移概率矩阵。 如下图1圆圈内是状态，箭头上的值是状态之间的转移概率。class是指上第几堂课，facebook指看facebook网页，pub指去酒吧，pass指通过考试，sleep指睡觉。例如处于class1有0.5的概率转移到class2，或者0.5的概率转移到facebook。 从而可以产生非常多的随机序列，例如C1 C2 C3 Pass Sleep或者C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep等。这些随机状态的序列就是马尔可夫过程。 马尔可夫奖励过程 Markov Reward Process马尔可夫奖励过程在马尔可夫过程的基础上增加了奖励 R 和衰减系数 γR是一个奖励函数。S 状态下的奖励是某一时刻(t)处在状态s下在下一个时刻 (t+1) 能获得的奖励期望：$R_{s} = E[R_{t+1} | S_{t} = s ]$ 衰减系数 Discount Factor: γ∈ [0, 1]，其远期利益具有一定的不确定性，符合人类对于眼前利益的追求等。 回报 Return定义：回报 $G_{t}$ 为在一个马尔可夫奖励链上从 t 时刻开始往后所有的奖励的有衰减的总和。公式如下：$G_t = R_{t+1}+\\gamma R_{t+2}+…=\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$其中衰减系数体现了未来的奖励在当前时刻的价值比例，在k+1时刻获得的奖励R在t时刻的体现出的价值是 $\\gamma^k R$ ，γ 接近0，则表明趋向于“近视”性评估；γ 接近1则表明偏重考虑远期的利益。 价值函数 Value Function状态值函数给出了某一状态或某一动作的长期价值。定义：一个马尔可夫奖励过程中的状态值函数为从该状态开始的马尔可夫链回报的期望：$v(s) = E [ G_{t} | S_{t} = s ]$ 价值函数的推导Bellman方程 - MRP先尝试用价值的定义公式来推导看看能得到什么：这个推导过程相对简单，仅在导出最后一行时，将 $G_{t+1}$ 变成了 $v(S_{t+1})$ 通过方程可以看出 v(s) 由两部分组成，一是该状态的即时奖励期望，即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关；另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。如果用s’表示s状态下一时刻任一可能的状态，那么Bellman方程可以写成：$v(s) = E[G_t|S_t=s]= E[R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3}+…) | S_t=s]= E[R_{t+1} + \\gamma G_{t+1} | S_t=s] = ER_s + \\gamma \\sum_{s’\\in S}P_{ss’}v(s’) $ 下图已经给出了 γ=1 时各状态的价值（该图没有文字说明 γ=1，根据视频讲解和前面图示以及状态方程的要求，γ 必须要确定才能计算），状态 $C_{3}$ 的价值可以通过状态 Pub 和 Pass 的价值以及他们之间的状态转移概率来计算：$4.3 = -2 + 1.0 ( 0.6 10 + 0.4 * 0.8 )$ Bellman方程的矩阵形式和求解实际上，计算复杂度是 $O(n^{3})$ ， n 是状态数量。因此直接求解仅适用于小规模的MRPs。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference，后文会逐步讲解这些方法。 马尔可夫决策过程 Markov Decision Process相较于马尔可夫奖励过程，马尔可夫决策过程多了一个动作（动作）集合A，它是这样的一个元组: 。看起来很类似马尔可夫奖励过程，但这里的P和R都与具体的动作a对应，而不像马尔可夫奖励过程那样仅对应于某个状态，A表示的是有限的动作的集合。具体的数学表达式如下：$P^a_{ss’} = P[S_{t+1}=s’|S_t=s,A_t=a]$$R^a_{s} = E[R_{t+1} | S_{t} = s,A_t=a ]$ 下图给出了一个可能的MDP的状态转化图。图中红色的文字表示的是采取的动作，而不是先前的状态名。对比之前的学生MRP示例可以发现，即时奖励与动作对应了，同一个状态下采取不同的动作得到的即时奖励是不一样的。由于引入了Action，容易与状态名混淆，因此此图没有给出各状态的名称；此图还把Pass和Sleep状态合并成一个终止状态；另外当选择”去查阅文献”这个动作时，主动进入了一个临时状态（图中用黑色小实点表示），随后被动的被环境按照其动力学分配到另外三个状态，也就是说此时Agent没有选择权决定去哪一个状态。 策略 Policy $\\pi$策略 $\\pi$是概率的集合或分布，其元素 $\\pi(a|s)$ 为对过程中的某一状态s采取可能的动作 a 的概率。用 $\\pi(a|s)$ 表示。 一个策略完整定义了 Agent 的动作方式，也就是说定义了 Agent 在各个状态下的各种可能的动作方式以及其概率的大小。Policy 仅和当前的状态有关，与历史信息无关；同时某一确定的Policy是静态的，与时间无关；但是 Agent 可以随着时间更新策略。 当给定一个MDP: M = &lt; S, A, P, R, $\\gamma$ &gt; 和一个策略 $\\pi$，那么状态序列 $S_{1},S_{2},…$ 是一个马尔可夫过程 &lt; $S$,$P^{\\pi}$&gt; :状态转移概率公式表示: $P^\\pi_{ss’}=\\sum_{a\\in A}\\pi(a|s)P^a_{ss’}$在执行策略 $\\pi$ 时，状态从 s 转移至 s’ 的概率等于一系列概率的和，这一系列概率指的是在执行当前策略时，执行某一个动作的概率与该动作能使状态从s转移至s’的概率的乘积。 同样的，状态和奖励序列 $S_{1}, R_{2}, S_{2}, R_{3}, S_{3}, … $是一个马尔可夫奖励过程 &lt; $S$, $P^{\\pi}$, $R^{\\pi}$, $\\gamma$ &gt;奖励函数表示：$R^\\pi_{s}=\\sum_{a\\in A}\\pi(a|s)R^a_{s}$当前状态s下执行某一指定策略得到的即时奖励是该策略下所有可能动作得到的奖励与该动作发生的概率的乘积的和。 策略 $\\pi$ 在MDP中的作用相当于 agent 可以在某一个状态时做出选择，进而有形成各种马尔可夫过程的可能，而且基于策略产生的每一个马尔可夫过程是一个马尔可夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。 基于策略 $\\pi$ 的价值函数 Value Function状态值函数 State-Value Function V定义 $v_\\pi(s)$ 是在 MDP 下的基于策略 $\\pi$ 的状态值函数，表示从状态s开始，遵循当前策略时所获得的收获的期望；或者说在执行当前策略 $\\pi$ 时，衡量个体处在状态s时的价值大小。数学表示如下：$v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]$ 注意策略是静态的、关于整体的概念，不随状态改变而改变；变化的是在某一个状态时，依据策略可能产生的具体动作，因为具体的动作是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同动作的概率。 动作值函数 Action-Value Function Q定义 $q_{\\pi}(s,a)$ 为动作值函数，表示在执行策略 $\\pi$ 时，对当前状态 s 执行某一具体动作 a 所能的到的收获的期望；或者说在遵循当前策略 $\\pi$ 时，衡量对当前状态执行动作 a 的价值大小。动作值函数一般都是与某一特定的状态相对应的。动作值函数的公式描述如下:$q_{\\pi}(s,a)= E_{\\pi}[G_t|S_t=s, A_t=a]$ 由于策略$\\pi(a|s)$是可以改变的，因此两个值函数的取值不像MRP一样是固定的，那么就能从不同的取值中找到一个最大值即最优值函数(这节课没有讲如何求解)。MDP需要解决的问题并不是每一步到底会获得多少累积reward，而是找到一个最优的解决方案。 Bellman期望方程 Bellman Expectation Equation根据这两个值函数的定义，它们之间的关系表示为 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s,a)$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’)$ 第二个式子是说当选择一个action之后，转移到不同状态下之后获取的reward之和是多少。将两个式子互相代入，可以得到如下的Bellman期望方程。 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)(R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’))$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^a\\sum_{a’\\in A}\\pi(a’|s’)q_{\\pi}(s’,a’)$ 下图解释了红色空心圆圈状态的状态价值是如何计算的，遵循的策略随机策略，即所有可能的动作有相同的几率被选择执行。和MRP类似的,我们也可以得到矩阵形式和求解 最优价值函数最优状态值函数 $v$ 指的是在从所有策略产生的状态值函数中，选取使状态 s 值最大的函数：$v_* = \\max \\limits_{\\pi} v_{\\pi}(s)$ 类似的，最优动作值函数 $q (s,a)$ 指的是从所有策略产生的动作值函数中，选取是状态动作对 &lt;$s$, $a$&gt; 价值最大的函数：$q_* (s,a) = \\max \\limits_{\\pi} q_{\\pi}(s,a)$ 最优价值函数明确了MDP的最优可能表现，当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。 最优策略当对于任何状态 s，遵循策略 $\\pi$ 的价值不小于遵循策略 $\\pi$’ 下的价值，则策略 $\\pi$ 优于策略 $\\pi$’：$\\pi \\geq \\pi^{‘} if v_{\\pi}(s)\\geq v_{\\pi^{‘}}(s) ,\\forall s $定理 对于任何MDP，下面几点成立： 存在一个最优策略，比任何其他策略更好或至少相等； 所有的最优策略有相同的最优价值函数； 所有的最优策略具有相同的动作值函数。 寻找最优策略可以通过最大化最优动作值函数来找到最优策略：对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优动作值函数，则表明我们找到了最优策略。 Bellman最优方程 Bellman Optimality Equation针对 $v$ 一个状态的最优价值等于从该状态出发采取的所有动作产生的动作价值中最大的那个动作价值：针对 $q$，在某个状态 s 下，采取某个动作的最优价值由2部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和： 组合起来，得到Bellman最优方程: 满足bellman最优方程，意味着找到了最优策略。也就是$v_\\pi (s) = \\max \\limits_{a} q_{\\pi}(s,a)$，也就是不需要在进行策略改进。 求解Bellman最优方程Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。后续会逐步讲解展开。 MDP延伸——Extensions to MDPs","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 1 Introduction","date":"2017-08-14T13:01:30.000Z","path":"2017/08/14/David_Silver_RL_1/","text":"强化学习的特点(不同于其他机器学习)： 没有监督数据、只有奖励信号 奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。 时间（序列）是一个重要因素 当前的行为影响后续接收到的数据 The RL Problem奖励 Reward 一个Reward $R_{t}$ 是信号的反馈，是一个标量 它反映 Agent 在 t 时刻做得怎么样 Agent 的工作就是最大化累计奖励 强化学习主要基于这样的”奖励假设”：所有问题解决的目标都可以被描述成最大化累积奖励。 序列决策 Sequential Decision Making 目标：选择一系列的 Action 以最大化未来的总体奖励 这些 Action 可能是一个长期的序列 奖励可能而且通常是延迟的 有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励 个体和环境 Agent &amp; Environment可以从个体和环境两方面来描述强化学习问题。在 t 时刻，Agent 可以： 做出一个行为 $A_{t}$ 有一个对于环境的观察评估 $O_{t}$ 从环境得到一个奖励信号 $R_{t}$ Environment 可以： 接收 Agent 的动作 $A_{t}$ 更新环境信息，同时使得Agent可以得到下一个观测 $O_{t+1}$ 给Agent一个奖励信号 $R_{t+1}$ 历史和状态 History &amp; State历史 历史是观测、行为、奖励的序列： $H_{t} = O_{1}, R_{1}, A_{1},…, O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}$ 状态 状态是所有能够决定将来的已有的信息，是关于历史的一个函数：$S_{t} = f(H_{t})$ 环境状态 Environment State 是环境的私有 representation 包括环境用来决定下一个观测/奖励的所有数据 通常对Agent并不完全可见，也就是Agent有时候并不知道环境状态的所有细节 即使有时候环境状态对Agent可以是完全可见的，这些信息也可能包含着一些无关信息 个体状态 Agent State 是Agent的内部representation 包括Agent可以使用的、决定未来动作的所有信息 Agent State是强化学习算法可以利用的信息 它可以是历史的一个函数： $S^{a}_{t} = f(H_{t})$ 信息状态 Information State包括历史上所有有用的信息，又称Markov状态。也就是说: 如果信息状态是可知的，那么历史可以丢弃，仅需要 t 时刻的信息状态就可以了。例如：环境状态是Markov的，因为环境状态是环境包含了环境决定下一个观测/奖励的所有信息 同样，（完整的）历史 $H_{t}$ 也是Markov的。 完全可观测的环境 Fully Observable Environments Agent能够直接观测到环境状态: $O_{t} = S^{a}_{t} = S^{e}_{t}$ 正式地说，这种问题是一个马尔可夫决策过程（Markov Decision Process， MDP） 部分可观测的环境 Partially Observable Environments Agent 间接观测环境。举了几个例子： 一个可拍照的机器人Agent对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一； 一个交易员只能看到当前的交易价格； 一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。 在这种条件下：个体状态 ≠ 环境状态 正式地说，这种问题是一个部分可观测马尔可夫决策过程 (POMDP)。Agent 必须构建它自己的状态 representation $S^{a}_{t}$，比如： 记住完整的历史： $S^{a}_{t} = H_{t}$ 这种方法比较原始、幼稚。还有其他办法，例如 ： Beliefs of environment state：此时虽然 Agent 不知道环境状态到底是什么样，但Agent可以利用已有经验（数据），用各种 Agent 已知状态的概率分布作为当前时刻的 Agent 状态的呈现：$S^{a}_{t} = (P[S^e_t=s^1],…,P[S^e_t=s^n])$ Recurrent neural network：不需要知道概率，只根据当前的Agent状态以及当前时刻Agent的观测，送入循环神经网络(RNN)中得到一个当前Agent状态的呈现：$S^{a}_{t} = \\sigma(S^e_{t-1}W_s + O_tW_o)$ Agent的主要组成部分强化学习中的Agent可以由以下三个组成部分中的一个或多个组成： 1. 策略 Policy策略是决定Agent行为的机制。是从状态到行为的一个映射，可以是确定性的，也可以是不确定性的。 确定的 Policy : $a=\\pi(s)$ 随机 Policy : $\\pi(a|s) = P[A_t=a|S_t=s]$ 2. 价值函数 Value Function 是一个对未来奖励的预测 用来评价当前状态的好坏程度 当面对两个不同的状态时，Agent可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。同时，一个价值函数是基于某一个特定策略的，不同的策略下同一状态的价值并不相同。某一策略下的价值函数用下式表示：$V_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+…|S_t=s]$ 3. 模型 ModelAgent对环境的一个建模，它体现了Agent是如何思考环境运行机制的（how the agent think what the environment was.），Agent希望模型能模拟环境与Agent的交互机制。 模型至少要解决两个问题：一是状态转化概率，即预测下一个可能状态发生的概率：$P^a_{s s^{‘}} = P[S_{t+1}=s^{‘}|S_t=s,A_t=a]$ 另一项工作是预测可能获得的即时奖励：$R^a_{s} = E[R_{t+1}|S_t=s,A_t=a]$ 模型并不是构建一个Agent所必需的，很多强化学习算法中Agent并不试图（依赖）构建一个模型。 注：模型仅针对Agent而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定Agent下一个状态和所得的即时奖励。 Agent的分类解决强化学习问题，Agent可以有多种工具组合，比如通过建立对状态的价值的估计来解决问题，或者通过直接建立对策略的估计来解决问题。这些都是Agent可以使用的工具箱里的工具。因此，根据Agent内包含的“工具”进行分类，可以把Agent分为如下三类： 仅基于价值函数的 Value Based：在这样的Agent中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。 仅直接基于策略的 Policy Based：这样的Agent中行为直接由策略函数产生，Agent并不维护一个对各状态价值的估计函数。 演员-评判家形式 Actor-Critic：Agent既有价值函数、也有策略函数。两者相互结合解决问题。 此外，根据Agent在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类： Model Free:这类Agent并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。 Model Based：Agent尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。 学习和规划 Learning &amp; Planning学习： 环境初始时是未知的 Agent不知道环境如何工作 Agent通过与环境进行交互，逐渐改善其行为策略。 规划: 环境如何工作对于Agent是已知或近似已知的 Agent并不与环境发生实际的交互，而是利用其构建的模型进行计算 在此基础上改善其行为策略。 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。 探索和利用 Exploration &amp; Exploitation强化学习是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。 在这样的学习过程中，就会有一个在Exploration和Exploitation之间的权衡，前者是说会放弃一些已知的reward信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么action让reward比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让reward更大，即Exploration希望能够探索更多关于environment的信息。而后者是指根据已知的信息最大化reward。例如，在选择一个餐馆时，Exploitation会选择你最喜欢的餐馆，而Exploration会尝试选择一个新的餐馆。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"从Fictitious Play 到 NFSP","date":"2017-07-27T13:13:00.000Z","path":"2017/07/27/从Fictitious Play 到 NFSP/","text":"博弈论Normal-form game在博弈论中，Normal-form game是对game的一种描述。Normal-form game通过矩阵来表示game。下图就是一个payoff矩阵：Normal-form game是一种静态模型，这个模型假设每个player仅选择一次action或策略。Normal-form game适用于描述不需要考虑博弈进程的完全信息静态(Complete information static)博弈。 静态博弈players同时进行决策 动态博弈players的决策顺序有先后 完全信息(Complete information)在完全信息博弈中，players的游戏结构和players的payoff矩阵是众所周知的，但players可能看不到其他players所做的所有动作。 完美信息(Perfect information)在完美信息博弈中，每个player可以观测到其他players的动作，但可能缺乏关于他人的payoff或游戏结构的信息。 Extensive-form gameExtensive-form game是一种动态模型。通过使用树这种图形来表示game。不仅给出了博弈结果，还描述了博弈过程，如给出博弈中参与人的行动顺序，以及决策环境和行动空间等。完美信息下的Extensive-form game:If player 1 plays D, player 2 will play U’ to maximise his payoff and so player 1 will only receive 1. However, if player 1 plays U, player 2 maximises his payoff by playing D’ and player 1 receives 2. Player 1 prefers 2 to 1 and so will play U and player 2 will play D’ 非完美信息下的Extensive-form game: 信息集 是一组决策节点： 集合中的每个节点都只属于一个player； 当player到达信息集时，player不能区分信息集中的节点。即，如果信息集包含多个节点，则该集合所属的player不知道集合中的哪个节点已经到达。 下图的虚线连接的两个节点2代表player2的一个信息集，player2不知道player1采取的是U还是D。补充：在扑克游戏中，player不知道的是对方的手牌信息而不是这个例子中的动作。player 2 cannot observe player 1’s move. Player 1 would like to fool player 2 into thinking he has played U when he has actually played D so that player 2 will play D’ and player 1 will receive 3. ref : https://en.wikipedia.org/wiki/Extensive-form_game Extensive-form game和Normal-form game的转换下图是Extensive-form game的一个例子：player1有三个信息集，每个信息集里面有两种行动，所以player1有8种策略；player2有两个信息集，每个信息集里面有两种行动，所以player2有4种策略。所以转换成Normal-form game是一个横8纵4的矩阵. FSP in Extensive-Form GamesExtensive-Form 定义扩充 ChanceChance被认为是一个特别的player，遵循一个固定的随机策略，决定了chance节点进入每个分支的概率分布。（如扑克游戏中的发牌）。 Information State我们假设游戏具有perfect recall，即每个player当前的信息状态u隐含了之前的信息状态和行动序列: Behavioural Strategy在给定Information State时，动作action的概率分布。Behavioural Strategy 输出player i在信息状态u下，动作的概率分布。Normal-Form 定义扩充 Pure Strategyplayer在可能遇到的所有情况下都有一个确定的动作action。(payoff矩阵中的每一行和列都是一个Pure Strategy) Mixed StrategyMixed Strateg是Player i的Pure Strategy的概率分布。 Fictitious Play在normal form下：当前策略 = 现有策略和bestResponse的加权平均。 Realization-equivalence也就是说，Extensive-Form下的Behavioural Strategy和Normal-form下的Mixed Strategy存在一种等价。Extensive-Form也可以使用Normal-form下的Fictitious Play来求纳什均衡解。 Extensive-Form Fictitious Play 代表到信息状态u的sequence = sequence 中每个节点选择每个action的概率乘积 下图是对Lemma 6的一个计算的验证。 我们通过Theorem7更新Behavioural Strategy，可以收敛到纳什均衡。 具体算法XFP： 计算bestResponse 更新策略，使用Theorem7 重复上面的过程 Fictitious Self-PlayXFP会有维度灾难。generalised weakened fictitious play只需要近似bestResponse，甚至允许更新中的一些扰动。 Fictitious Self-Play： 使用强化学习去计算bestResponse 使用监督学习更新策略 样本的采集 GENERATEDAT()方法需要在理解一下。 Neural Fictitious Self-Play强化学习和监督学习都使用神经网络去学习出一个函数。 FSP中样本采集的混合策略 ，在NSFP中是函数，不能简单相加。转换成 ，使用概率完成上面式子中的加操作。 REF： Topics in Algorithmic Game Theory February 8, 2010 Lecturer: Constantinos Daskalakis Lecture 2 Fictitious Self-Play in Extensive-Form Games Deep Reinforcement Learning from Self-play in imperfect-information games","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gyh75520.github.io/tags/博弈论/"},{"name":"ML","slug":"ML","permalink":"https://gyh75520.github.io/tags/ML/"}]},{"title":"opencv3 --python3 mac下配置","date":"2017-07-22T11:38:00.000Z","path":"2017/07/22/opencv3 --python3 mac配置/","text":"安装Anaconda把Anaconda安装的python版本设置为默认启动版本修改.bash_profile文件：123456789101112# Setting PATH for Python 2.7 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;&quot; export PATH # Setting PATH for Python 3.4 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.4/bin:$&#123;PATH&#125;&quot; export PATH # added by howard export PATH=&quot;/usr/local/anaconda3/bin:$PATH&quot; PATH替换成自己的路径 根据该脚本，先会去找 /usr/local/anaconda3/bin ，发现有，就为当前路径下的解释器环境，并执行。—— 所以，想设置python的版本，直接把你想添加的路径export上去，并放在后面。ref: http://blog.csdn.net/u010692239/article/details/52701626 安装opencv1brew install opencv3 --with-python3 安装好后，在最后它会提示你如果想要Python也能调用opencv接口的话，需要执行下面命令：1234If you need Python to find bindings for this keg-only formula, run:```bashecho /usr/local/opt/opencv3/lib/python2.7/site-packages &gt;&gt; /usr/local/lib/python2.7/site-packages/opencv3.pth echo打印输出，&gt;&gt;重定向，执行完这句，可以在/usr/local/lib/python2.7/site-packages/目录下得到一个文件opencv3.pth。但是我们来看看它所放置的目录，这个目录是系统自带的Python目录，而我们使用的Anaconda里的Python，所以你需要将其重定向输出的路径改到Anaconda中Python目录下，比如我的：1echo /usr/local/opt/opencv3/lib/python3.6/site-packages &gt;&gt; /usr/local/anaconda3/lib/python3.6/site-packages/opencv3.pth ref: http://blog.csdn.net/willard_yuan/article/details/46721831 安装完成之后你一定会迫不及待的去测试一下：123pythonimport cv2RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa 如果出现上述错误，证明你numpy的版本不对，升级版本：1pip install numpy --upgrade","tags":[{"name":"opencv3","slug":"opencv3","permalink":"https://gyh75520.github.io/tags/opencv3/"},{"name":"mac","slug":"mac","permalink":"https://gyh75520.github.io/tags/mac/"}]},{"title":"HEXO+Github搭建博客","date":"2016-10-17T15:33:00.000Z","path":"2016/10/17/first/","text":"hexo是一款基于Node.js的静态博客框架。 之前是想着写博客，一方面是给自己做笔记，可以提升自己的写作、总结能力，这里记录一下linux下面Hexo搭建的步骤。 配置环境 安装Node.js 安装Git Github新建仓库和连接新建repository(仓库)登陆Github账号后，点击右上角的“+”号按钮，选择“New repository” 在Create a new repository界面填写仓库名必须为【your_user_name.github.io】固定写法 填写完成点Create repository创建完成 生成SSH Keys：我们如何让本地git项目与远程的github建立联系？这时候就要用到SSH Keys 使用ssh-keygen命令生成密钥对 1ssh-keygen -t rsa -C\"这里是你申请Github账号时的邮箱\" 然后系统会要你输入密码：（我们输入的密码会在你提交项目的时候使用） 12Enter passphrase (emptyforno passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; （终端提示生成的文件路径）找到你生成的密钥找到id_rsa.pub用终端进入编辑，复制密钥。 添加你的SSH Key到ssh-agent登陆Github,点击右侧用户按钮，选择Settings 点击 Add SSH key 按钮，将复制的密钥粘贴到 Key 栏 测试能不能链接成功1ssh -T git@github.com 执行结果 Permanently addedtheRSA host keyforIP address ‘192.30.252.130’tothelistofknown hosts.Are you sure you wanttocontinueconnecting (yes/no)?&lt;输入yes&gt;Hi username! You’ve successfully authenticated,butGitHubdoesnot 现在你已经可以通过SSH链接到Github了 正式安装HexoNode和Git都安装好后,首先创建一个文件夹,如blog,用户存放hexo的配置文件,然后进入blog里安装Hexo。我安装的是hexo 3.0以上的版本。 执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 初始化然后，执行init命令初始化hexo,命令： 1hexo init 好啦，至此，全部安装工作已经完成！blog就是你的博客根目录，所有的操作都在里面进行。 生成静态页面 1hexo generate（hexo g也可以） 本地启动 启动本地服务，进行文章预览调试，命令： 1hexo server 浏览器输入http://localhost:4000 然后建立关联，blog文件夹下有： _config.yml node_modules public source db.json package.json scaffolds themes 现在我们需要修改_config.yml文件，翻到最下面，改成像我这样：1234567deploy: type: git repo: https://github.com/**gyh75520**/**gyh75520**.github.io.git branch: master 加粗的部分替换成前面的配置的your_user_name 然后执行命令： 1npm install hexo-deployer-git --save 然后，执行配置命令： 1hexo deploy 到这里项目已经部署到了github pages上 访问https://your_user_name.github.io 查看自己的博客吧O(∩_∩)O 备注：没有权限的话记得在命令前加上sudo Mac篇在mac配置，执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 报错:1npm ERR! 解决：把官方的源替换成淘宝的源，替换的方法12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install -g hexo-cli Hexo下mathjax的转义问题我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax,可以解析网页上的数学公式，与hexo的结合也很简单，可以手动加入js，或者直接使用hexo-math插件.大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候: — 的转义，在markdown中，_是斜体，但是在latex中，却有下标的意思，就会出现问题。 \\\\\\的换行，在markdown中，\\\\\\会被转义为\\\\,这样也会影响影响mathjax对公式中的\\\\\\进行渲染 解决办法：参考 https://segmentfault.com/a/1190000007261752","tags":[{"name":"HEXO","slug":"HEXO","permalink":"https://gyh75520.github.io/tags/HEXO/"}]}]