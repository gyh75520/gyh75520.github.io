[{"title":"David Silver 强化学习 1","date":"2017-08-14T13:01:30.000Z","path":"2017/08/14/David_Silver_RL_1/","text":"强化学习的特点(不同于其他机器学习)： 没有监督数据、只有奖励信号 奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。 时间（序列）是一个重要因素 当前的行为影响后续接收到的数据 The RL Problem奖励 Reward 一个Reward $R_{t}$ 是信号的反馈，是一个标量 它反映 Agent 在 t 时刻做得怎么样 Agent 的工作就是最大化累计奖励 强化学习主要基于这样的”奖励假设”：所有问题解决的目标都可以被描述成最大化累积奖励。 序列决策 Sequential Decision Making 目标：选择一系列的 Action 以最大化未来的总体奖励 这些 Action 可能是一个长期的序列 奖励可能而且通常是延迟的 有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励 个体和环境 Agent &amp; Environment可以从个体和环境两方面来描述强化学习问题。在 t 时刻，Agent 可以： 做出一个行为 $A_{t}$ 有一个对于环境的观察评估 $O_{t}$ 从环境得到一个奖励信号 $R_{t}$ Environment 可以： 接收 Agent 的动作 $A_{t}$ 更新环境信息，同时使得Agent可以得到下一个观测 $O_{t+1}$ 给Agent一个奖励信号 $R_{t+1}$ 历史和状态 History &amp; State历史 历史是观测、行为、奖励的序列： $H_{t} = O_{1}, R_{1}, A_{1},…, O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}$ 状态 状态是所有能够决定将来的已有的信息，是关于历史的一个函数：$S_{t} = f(H_{t})$ 环境状态 Environment State 是环境的私有 representation 包括环境用来决定下一个观测/奖励的所有数据 通常对Agent并不完全可见，也就是Agent有时候并不知道环境状态的所有细节 即使有时候环境状态对Agent可以是完全可见的，这些信息也可能包含着一些无关信息 个体状态 Agent State 是Agent的内部representation 包括Agent可以使用的、决定未来动作的所有信息 Agent State是强化学习算法可以利用的信息 它可以是历史的一个函数： $S^{a}_{t} = f(H_{t})$ 信息状态 Information State包括历史上所有有用的信息，又称Markov状态。也就是说: 如果信息状态是可知的，那么历史可以丢弃，仅需要 t 时刻的信息状态就可以了。例如：环境状态是Markov的，因为环境状态是环境包含了环境决定下一个观测/奖励的所有信息 同样，（完整的）历史 $H_{t}$ 也是Markov的。 完全可观测的环境 Fully Observable Environments Agent能够直接观测到环境状态: $O_{t} = S^{a}_{t} = S^{e}_{t}$ 正式地说，这种问题是一个马尔可夫决策过程（Markov Decision Process， MDP） 部分可观测的环境 Partially Observable Environments Agent 间接观测环境。举了几个例子： 一个可拍照的机器人Agent对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一； 一个交易员只能看到当前的交易价格； 一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。 在这种条件下：个体状态 ≠ 环境状态 正式地说，这种问题是一个部分可观测马尔可夫决策过程 (POMDP)。Agent 必须构建它自己的状态 representation $S^{a}_{t}$，比如： 记住完整的历史： $S^{a}_{t} = H_{t}$ 这种方法比较原始、幼稚。还有其他办法，例如 ： Beliefs of environment state：此时虽然 Agent 不知道环境状态到底是什么样，但Agent可以利用已有经验（数据），用各种 Agent 已知状态的概率分布作为当前时刻的 Agent 状态的呈现：$S^{a}_{t} = (P[S^e_t=s^1],…,P[S^e_t=s^n])$ Recurrent neural network：不需要知道概率，只根据当前的Agent状态以及当前时刻Agent的观测，送入循环神经网络(RNN)中得到一个当前Agent状态的呈现：$S^{a}_{t} = \\sigma(S^e_{t-1}W_s + O_tW_o)$ Agent的主要组成部分强化学习中的Agent可以由以下三个组成部分中的一个或多个组成： 1. 策略 Policy策略是决定Agent行为的机制。是从状态到行为的一个映射，可以是确定性的，也可以是不确定性的。 确定的 Policy : $a=\\pi(s)$ 随机 Policy : $\\pi(a|s) = P[A_t=a|S_t=s]$ 2. 价值函数 Value Function 是一个对未来奖励的预测 用来评价当前状态的好坏程度 当面对两个不同的状态时，Agent可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。同时，一个价值函数是基于某一个特定策略的，不同的策略下同一状态的价值并不相同。某一策略下的价值函数用下式表示：$V_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+…|S_t=s]$ 3. 模型 ModelAgent对环境的一个建模，它体现了Agent是如何思考环境运行机制的（how the agent think what the environment was.），Agent希望模型能模拟环境与Agent的交互机制。 模型至少要解决两个问题：一是状态转化概率，即预测下一个可能状态发生的概率：$P^a_{s s^{‘}} = P[S_{t+1}=s^{‘}|S_t=s,A_t=a]$ 另一项工作是预测可能获得的即时奖励：$R^a_{s} = E[R_{t+1}|S_t=s,A_t=a]$ 模型并不是构建一个Agent所必需的，很多强化学习算法中Agent并不试图（依赖）构建一个模型。 注：模型仅针对Agent而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定Agent下一个状态和所得的即时奖励。 Agent的分类解决强化学习问题，Agent可以有多种工具组合，比如通过建立对状态的价值的估计来解决问题，或者通过直接建立对策略的估计来解决问题。这些都是Agent可以使用的工具箱里的工具。因此，根据Agent内包含的“工具”进行分类，可以把Agent分为如下三类： 仅基于价值函数的 Value Based：在这样的Agent中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。 仅直接基于策略的 Policy Based：这样的Agent中行为直接由策略函数产生，Agent并不维护一个对各状态价值的估计函数。 演员-评判家形式 Actor-Critic：Agent既有价值函数、也有策略函数。两者相互结合解决问题。 此外，根据Agent在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类： Model Free:这类Agent并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。 Model Based：Agent尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。 探索和利用 Exploration &amp;&amp; Exploitation强化学习是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。 在这样的学习过程中，就会有一个在Exploration和Exploitation之间的权衡，前者是说会放弃一些已知的reward信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么action让reward比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让reward更大，即Exploration希望能够探索更多关于environment的信息。而后者是指根据已知的信息最大化reward。例如，在选择一个餐馆时，Exploitation会选择你最喜欢的餐馆，而Exploration会尝试选择一个新的餐馆。","tags":[]},{"title":"从Fictitious Play 到 NFSP","date":"2017-07-27T13:13:00.000Z","path":"2017/07/27/从Fictitious Play 到 NFSP/","text":"博弈论Normal-form game在博弈论中，Normal-form game是对game的一种描述。Normal-form game通过矩阵来表示game。下图就是一个payoff矩阵：Normal-form game是一种静态模型，这个模型假设每个player仅选择一次action或策略。Normal-form game适用于描述不需要考虑博弈进程的完全信息静态(Complete information static)博弈。 静态博弈players同时进行决策 动态博弈players的决策顺序有先后 完全信息(Complete information)在完全信息博弈中，players的游戏结构和players的payoff矩阵是众所周知的，但players可能看不到其他players所做的所有动作。 完美信息(Perfect information)在完美信息博弈中，每个player可以观测到其他players的动作，但可能缺乏关于他人的payoff或游戏结构的信息。 Extensive-form gameExtensive-form game是一种动态模型。通过使用树这种图形来表示game。不仅给出了博弈结果，还描述了博弈过程，如给出博弈中参与人的行动顺序，以及决策环境和行动空间等。完美信息下的Extensive-form game:If player 1 plays D, player 2 will play U’ to maximise his payoff and so player 1 will only receive 1. However, if player 1 plays U, player 2 maximises his payoff by playing D’ and player 1 receives 2. Player 1 prefers 2 to 1 and so will play U and player 2 will play D’ 非完美信息下的Extensive-form game: 信息集 是一组决策节点： 集合中的每个节点都只属于一个player； 当player到达信息集时，player不能区分信息集中的节点。即，如果信息集包含多个节点，则该集合所属的player不知道集合中的哪个节点已经到达。 下图的虚线连接的两个节点2代表player2的一个信息集，player2不知道player1采取的是U还是D。补充：在扑克游戏中，player不知道的是对方的手牌信息而不是这个例子中的动作。player 2 cannot observe player 1’s move. Player 1 would like to fool player 2 into thinking he has played U when he has actually played D so that player 2 will play D’ and player 1 will receive 3. ref : https://en.wikipedia.org/wiki/Extensive-form_game Extensive-form game和Normal-form game的转换下图是Extensive-form game的一个例子：player1有三个信息集，每个信息集里面有两种行动，所以player1有8种策略；player2有两个信息集，每个信息集里面有两种行动，所以player2有4种策略。所以转换成Normal-form game是一个横8纵4的矩阵. FSP in Extensive-Form GamesExtensive-Form 定义扩充 ChanceChance被认为是一个特别的player，遵循一个固定的随机策略，决定了chance节点进入每个分支的概率分布。（如扑克游戏中的发牌）。 Information State我们假设游戏具有perfect recall，即每个player当前的信息状态u隐含了之前的信息状态和行动序列: Behavioural Strategy在给定Information State时，动作action的概率分布。Behavioural Strategy 输出player i在信息状态u下，动作的概率分布。Normal-Form 定义扩充 Pure Strategyplayer在可能遇到的所有情况下都有一个确定的动作action。(payoff矩阵中的每一行和列都是一个Pure Strategy) Mixed StrategyMixed Strateg是Player i的Pure Strategy的概率分布。 Fictitious Play在normal form下：当前策略 = 现有策略和bestResponse的加权平均。 Realization-equivalence也就是说，Extensive-Form下的Behavioural Strategy和Normal-form下的Mixed Strategy存在一种等价。Extensive-Form也可以使用Normal-form下的Fictitious Play来求纳什均衡解。 Extensive-Form Fictitious Play 代表到信息状态u的sequence = sequence 中每个节点选择每个action的概率乘积 下图是对Lemma 6的一个计算的验证。 我们通过Theorem7更新Behavioural Strategy，可以收敛到纳什均衡。 具体算法XFP： 计算bestResponse 更新策略，使用Theorem7 重复上面的过程 Fictitious Self-PlayXFP会有维度灾难。generalised weakened fictitious play只需要近似bestResponse，甚至允许更新中的一些扰动。 Fictitious Self-Play： 使用强化学习去计算bestResponse 使用监督学习更新策略 样本的采集 GENERATEDAT()方法需要在理解一下。 Neural Fictitious Self-Play强化学习和监督学习都使用神经网络去学习出一个函数。 FSP中样本采集的混合策略 ，在NSFP中是函数，不能简单相加。转换成 ，使用概率完成上面式子中的加操作。 REF： Topics in Algorithmic Game Theory February 8, 2010 Lecturer: Constantinos Daskalakis Lecture 2 Fictitious Self-Play in Extensive-Form Games Deep Reinforcement Learning from Self-play in imperfect-information games","tags":[{"name":"博弈论","slug":"博弈论","permalink":"https://gyh75520.github.io/tags/博弈论/"},{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"},{"name":"ML","slug":"ML","permalink":"https://gyh75520.github.io/tags/ML/"}]},{"title":"opencv3 --python3 mac下配置","date":"2017-07-22T11:38:00.000Z","path":"2017/07/22/opencv3 --python3 mac配置/","text":"安装Anaconda把Anaconda安装的python版本设置为默认启动版本修改.bash_profile文件：123456789101112# Setting PATH for Python 2.7 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;&quot; export PATH # Setting PATH for Python 3.4 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.4/bin:$&#123;PATH&#125;&quot; export PATH # added by howard export PATH=&quot;/usr/local/anaconda3/bin:$PATH&quot; PATH替换成自己的路径 根据该脚本，先会去找 /usr/local/anaconda3/bin ，发现有，就为当前路径下的解释器环境，并执行。—— 所以，想设置python的版本，直接把你想添加的路径export上去，并放在后面。ref: http://blog.csdn.net/u010692239/article/details/52701626 安装opencv1brew install opencv3 --with-python3 安装好后，在最后它会提示你如果想要Python也能调用opencv接口的话，需要执行下面命令：1234If you need Python to find bindings for this keg-only formula, run:```bashecho /usr/local/opt/opencv3/lib/python2.7/site-packages &gt;&gt; /usr/local/lib/python2.7/site-packages/opencv3.pth echo打印输出，&gt;&gt;重定向，执行完这句，可以在/usr/local/lib/python2.7/site-packages/目录下得到一个文件opencv3.pth。但是我们来看看它所放置的目录，这个目录是系统自带的Python目录，而我们使用的Anaconda里的Python，所以你需要将其重定向输出的路径改到Anaconda中Python目录下，比如我的：1echo /usr/local/opt/opencv3/lib/python3.6/site-packages &gt;&gt; /usr/local/anaconda3/lib/python3.6/site-packages/opencv3.pth ref: http://blog.csdn.net/willard_yuan/article/details/46721831 安装完成之后你一定会迫不及待的去测试一下：123pythonimport cv2RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa 如果出现上述错误，证明你numpy的版本不对，升级版本：1pip install numpy --upgrade","tags":[{"name":"opencv3","slug":"opencv3","permalink":"https://gyh75520.github.io/tags/opencv3/"},{"name":"mac","slug":"mac","permalink":"https://gyh75520.github.io/tags/mac/"}]},{"title":"HEXO+Github搭建博客","date":"2016-10-17T15:33:00.000Z","path":"2016/10/17/first/","text":"hexo是一款基于Node.js的静态博客框架。 之前是想着写博客，一方面是给自己做笔记，可以提升自己的写作、总结能力，这里记录一下linux下面Hexo搭建的步骤。 配置环境 安装Node.js 安装Git Github新建仓库和连接新建repository(仓库)登陆Github账号后，点击右上角的“+”号按钮，选择“New repository” 在Create a new repository界面填写仓库名必须为【your_user_name.github.io】固定写法 填写完成点Create repository创建完成 生成SSH Keys：我们如何让本地git项目与远程的github建立联系？这时候就要用到SSH Keys 使用ssh-keygen命令生成密钥对 1ssh-keygen -t rsa -C\"这里是你申请Github账号时的邮箱\" 然后系统会要你输入密码：（我们输入的密码会在你提交项目的时候使用） 12Enter passphrase (emptyforno passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; （终端提示生成的文件路径）找到你生成的密钥找到id_rsa.pub用终端进入编辑，复制密钥。 添加你的SSH Key到ssh-agent登陆Github,点击右侧用户按钮，选择Settings 点击 Add SSH key 按钮，将复制的密钥粘贴到 Key 栏 测试能不能链接成功1ssh -T git@github.com 执行结果 Permanently addedtheRSA host keyforIP address ‘192.30.252.130’tothelistofknown hosts.Are you sure you wanttocontinueconnecting (yes/no)?&lt;输入yes&gt;Hi username! You’ve successfully authenticated,butGitHubdoesnot 现在你已经可以通过SSH链接到Github了 正式安装HexoNode和Git都安装好后,首先创建一个文件夹,如blog,用户存放hexo的配置文件,然后进入blog里安装Hexo。我安装的是hexo 3.0以上的版本。 执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 初始化然后，执行init命令初始化hexo,命令： 1hexo init 好啦，至此，全部安装工作已经完成！blog就是你的博客根目录，所有的操作都在里面进行。 生成静态页面 1hexo generate（hexo g也可以） 本地启动 启动本地服务，进行文章预览调试，命令： 1hexo server 浏览器输入http://localhost:4000 然后建立关联，blog文件夹下有： _config.yml node_modules public source db.json package.json scaffolds themes 现在我们需要修改_config.yml文件，翻到最下面，改成像我这样：1234567deploy: type: git repo: https://github.com/**gyh75520**/**gyh75520**.github.io.git branch: master 加粗的部分替换成前面的配置的your_user_name 然后执行命令： 1npm install hexo-deployer-git --save 然后，执行配置命令： 1hexo deploy 到这里项目已经部署到了github pages上 访问https://your_user_name.github.io 查看自己的博客吧O(∩_∩)O 备注：没有权限的话记得在命令前加上sudo Mac篇在mac配置，执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 报错:1npm ERR! 解决：把官方的源替换成淘宝的源，替换的方法12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install -g hexo-cli Hexo下mathjax的转义问题我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax,可以解析网页上的数学公式，与hexo的结合也很简单，可以手动加入js，或者直接使用hexo-math插件.大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候: — 的转义，在markdown中，_是斜体，但是在latex中，却有下标的意思，就会出现问题。 \\\\\\的换行，在markdown中，\\\\\\会被转义为\\\\,这样也会影响影响mathjax对公式中的\\\\\\进行渲染 解决办法：参考 https://segmentfault.com/a/1190000007261752","tags":[{"name":"HEXO","slug":"HEXO","permalink":"https://gyh75520.github.io/tags/HEXO/"}]}]