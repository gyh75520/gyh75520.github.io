[{"title":"学徒学习 Apprenticeship learning via inverse reinforcement learning","date":"2017-09-15T13:01:30.000Z","path":"2017/09/15/Apprenticeship_learing/","text":"学徒学习是Ng（吴恩达）和Abbeel提出来的。学徒学习是这样：智能体从专家示例中学到回报函数，使得在该回报函数下所得到的最优策略在专家示例策略附近。 回报函数$R(s)$ 假设为：$R\\left(s\\right)=w^T\\cdot\\phi\\left(s\\right)$，其中$\\phi(s)$为映射特征的基函数，可以为多项式基底，也可以为傅里叶基底。文中是以线性函数为基底。逆向强化学习求的就是回报函数中的系数w。策略 $\\pi$ 的值函数为：$v_{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t R(s_t)]$将回报函数代入：$v_{\\pi}(s) = w^T E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$ 将上式右半部分定义为特征期望：$\\mu\\left(\\pi\\right)=E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$。需要注意的是，特征期望跟策略 $\\pi$ 有关，策略不同时，策略期望也不相同 当给定m条专家轨迹后，根据定义我们可以估计专家策略的特征期望为：$\\hat{\\mu}_E=\\frac{1}{m}\\Sigma_{i=1}^{m}\\Sigma_{t=0}^{\\infty}\\gamma^t\\phi\\left(s_{t}^{\\left(i\\right)}\\right)$其中，专家状态序列为专家轨迹:${ s_{0}^{(i)},s_{1}^{(i)},\\cdots }_{i=1}^{m}$ 逆向强化学习可以归结为如下问题： 找到一个策略，使得该策略的表现与专家策略相近。我们可以利用特征期望来表示一个策略的好坏，找到一个策略，使其表现与专家策略相近，其实就是找到一个策略 $\\tilde{\\pi}$ 的特征期望与专家策略的特征期望相近，即使如下不等式成立：$\\lVert\\mu\\left(\\tilde{\\pi}\\right)-\\mu_E\\rVert_2\\le\\epsilon$ 当该不等式成立时，对于任意的权重$\\lVert w\\rVert_1\\le 1$，值函数满足如下不等式： 算法 其中第二行的目标函数为: $t^{(i)}=\\max_{w:\\lVert w\\rVert_2\\le 1}\\min_{j\\in{0\\cdots(i-1)}}w^T(\\mu_E-\\mu^{(j)})$ 写成标准的优化形式为：注意，在进行第二行求解时，$\\mu^{\\left(j\\right)}$中的 $j\\in{0,1,\\cdots ,i-1}$是前i-1次迭代得到的最优策略。也就是说第i次求解参数时，i-1次迭代的策略是已知的。这时候的最优函数值t相当于专家策略 $\\mu_E$ 与i-1个迭代策略之间的最大边际。 我们可以从支持向量机的角度去理解。专家策略为一类，其他策略为另一类，参数的求解其实就是找一条超曲面将专家策略和其他策略区分开来。这个超平面使得两类之间的边际最大。 第四行是在第二行求出参数后，便有了回报函数$R=\\left(w^{\\left(i\\right)}\\right)^T\\phi$，利用该回报函数进行强化学习，从而得到该回报函数下的最优策略 $\\pi^{\\left(i\\right)}$。 总结学徒逆向强化学习方法分为两步:第一步在已经迭代得到的最优策略中，利用最大边际方法求出当前的回报函数的参数值；（该计算需要用到QP(二次规划)求解器或者\bSVM求解器。文中也给出了一种不\b使用\b求解器的简单算法。）第二步利用求出的回报函数的参数值进行正向强化学习方法求得当前最优的策略，然后重复第一步。 需要注意的是，$\\phi(s)$ 中输入的 s \b为 i 个特征：$s^1,s^2,…,s^i$，if 第 i 个特征存在，$s^i = 1$，else $s^i = 0$。又因为$\\lVert w\\rVert_1\\le 1$，所以$R\\le 1$。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 2 MDP","date":"2017-08-18T13:01:30.000Z","path":"2017/08/18/David_Silver_RL_2/","text":"在强化学习中，马尔可夫决策过程（Markov decision process, MDP）是对完全可观测的环境进行描述的，也就是说观测到的状态内容完整地决定了决策的需要的特征。几乎所有的强化学习问题都可以转化为MDP。本讲是理解强化学习问题的理论基础。 马尔可夫过程 Markov Process马尔可夫性 Markov Property 某一状态信息包含了所有相关的历史 只要当前状态可知，历史信息 history 就可以被丢弃 当前状态就可以决定未来 The future is independent of the past given the present即该状态具有马尔可夫性。 可以用下面的状态转移概率公式来描述马尔可夫性：$P_{ss’} = P[S_{t+1}=s’|S_t=s]$ 下面的状态转移矩阵定义了所有状态的转移概率：式中n为状态数量，矩阵中每一行元素之和为1. 马尔可夫过程 Markov Process马尔可夫过程 又叫马尔可夫链(Markov Chain)，它是一个无记忆的随机过程，可以用一个元组表示，其中S是有限数量的状态集，P是状态转移概率矩阵。 如下图1圆圈内是状态，箭头上的值是状态之间的转移概率。class是指上第几堂课，facebook指看facebook网页，pub指去酒吧，pass指通过考试，sleep指睡觉。例如处于class1有0.5的概率转移到class2，或者0.5的概率转移到facebook。 从而可以产生非常多的随机序列，例如C1 C2 C3 Pass Sleep或者C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep等。这些随机状态的序列就是马尔可夫过程。 马尔可夫奖励过程 Markov Reward Process马尔可夫奖励过程在马尔可夫过程的基础上增加了奖励 R 和衰减系数 γR是一个奖励函数。S 状态下的奖励是某一时刻(t)处在状态s下在下一个时刻 (t+1) 能获得的奖励期望：$R_{s} = E[R_{t+1} | S_{t} = s ]$ 衰减系数 Discount Factor: γ∈ [0, 1]，其远期利益具有一定的不确定性，符合人类对于眼前利益的追求等。 回报 Return定义：回报 $G_{t}$ 为在一个马尔可夫奖励链上从 t 时刻开始往后所有的奖励的有衰减的总和。公式如下：$G_t = R_{t+1}+\\gamma R_{t+2}+…=\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$其中衰减系数体现了未来的奖励在当前时刻的价值比例，在k+1时刻获得的奖励R在t时刻的体现出的价值是 $\\gamma^k R$ ，γ 接近0，则表明趋向于“近视”性评估；γ 接近1则表明偏重考虑远期的利益。 价值函数 Value Function状态值函数给出了某一状态或某一动作的长期价值。定义：一个马尔可夫奖励过程中的状态值函数为从该状态开始的马尔可夫链回报的期望：$v(s) = E [ G_{t} | S_{t} = s ]$ 价值函数的推导Bellman方程 - MRP先尝试用价值的定义公式来推导看看能得到什么：这个推导过程相对简单，仅在导出最后一行时，将 $G_{t+1}$ 变成了 $v(S_{t+1})$ 通过方程可以看出 v(s) 由两部分组成，一是该状态的即时奖励期望，即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关；另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。如果用s’表示s状态下一时刻任一可能的状态，那么Bellman方程可以写成：$v(s) = E[G_t|S_t=s]= E[R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3}+…) | S_t=s]= E[R_{t+1} + \\gamma G_{t+1} | S_t=s] = ER_s + \\gamma \\sum_{s’\\in S}P_{ss’}v(s’) $ 下图已经给出了 γ=1 时各状态的价值（该图没有文字说明 γ=1，根据视频讲解和前面图示以及状态方程的要求，γ 必须要确定才能计算），状态 $C_{3}$ 的价值可以通过状态 Pub 和 Pass 的价值以及他们之间的状态转移概率来计算：$4.3 = -2 + 1.0 ( 0.6 10 + 0.4 * 0.8 )$ Bellman方程的矩阵形式和求解实际上，计算复杂度是 $O(n^{3})$ ， n 是状态数量。因此直接求解仅适用于小规模的MRPs。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference，后文会逐步讲解这些方法。 马尔可夫决策过程 Markov Decision Process相较于马尔可夫奖励过程，马尔可夫决策过程多了一个动作（动作）集合A，它是这样的一个元组: 。看起来很类似马尔可夫奖励过程，但这里的P和R都与具体的动作a对应，而不像马尔可夫奖励过程那样仅对应于某个状态，A表示的是有限的动作的集合。具体的数学表达式如下：$P^a_{ss’} = P[S_{t+1}=s’|S_t=s,A_t=a]$$R^a_{s} = E[R_{t+1} | S_{t} = s,A_t=a ]$ 下图给出了一个可能的MDP的状态转化图。图中红色的文字表示的是采取的动作，而不是先前的状态名。对比之前的学生MRP示例可以发现，即时奖励与动作对应了，同一个状态下采取不同的动作得到的即时奖励是不一样的。由于引入了Action，容易与状态名混淆，因此此图没有给出各状态的名称；此图还把Pass和Sleep状态合并成一个终止状态；另外当选择”去查阅文献”这个动作时，主动进入了一个临时状态（图中用黑色小实点表示），随后被动的被环境按照其动力学分配到另外三个状态，也就是说此时Agent没有选择权决定去哪一个状态。 策略 Policy $\\pi$策略 $\\pi$是概率的集合或分布，其元素 $\\pi(a|s)$ 为对过程中的某一状态s采取可能的动作 a 的概率。用 $\\pi(a|s)$ 表示。 一个策略完整定义了 Agent 的动作方式，也就是说定义了 Agent 在各个状态下的各种可能的动作方式以及其概率的大小。Policy 仅和当前的状态有关，与历史信息无关；同时某一确定的Policy是静态的，与时间无关；但是 Agent 可以随着时间更新策略。 当给定一个MDP: M = 和一个策略 $\\pi$，那么状态序列 $S_{1},S_{2},…$ 是一个马尔可夫过程 &lt; $S$,$P^{\\pi}$&gt; :状态转移概率公式表示: $P^\\pi_{ss’}=\\sum_{a\\in A}\\pi(a|s)P^a_{ss’}$在执行策略 $\\pi$ 时，状态从 s 转移至 s’ 的概率等于一系列概率的和，这一系列概率指的是在执行当前策略时，执行某一个动作的概率与该动作能使状态从s转移至s’的概率的乘积。 同样的，状态和奖励序列 $S_{1}, R_{2}, S_{2}, R_{3}, S_{3}, … $是一个马尔可夫奖励过程 &lt; $S$, $P^{\\pi}$, $R^{\\pi}$, $\\gamma$ &gt;奖励函数表示：$R^\\pi_{s}=\\sum_{a\\in A}\\pi(a|s)R^a_{s}$当前状态s下执行某一指定策略得到的即时奖励是该策略下所有可能动作得到的奖励与该动作发生的概率的乘积的和。 策略 $\\pi$ 在MDP中的作用相当于 agent 可以在某一个状态时做出选择，进而有形成各种马尔可夫过程的可能，而且基于策略产生的每一个马尔可夫过程是一个马尔可夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。 基于策略 $\\pi$ 的价值函数 Value Function状态值函数 State-Value Function V定义 $v_\\pi(s)$ 是在 MDP 下的基于策略 $\\pi$ 的状态值函数，表示从状态s开始，遵循当前策略时所获得的收获的期望；或者说在执行当前策略 $\\pi$ 时，衡量个体处在状态s时的价值大小。数学表示如下：$v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]$ 注意策略是静态的、关于整体的概念，不随状态改变而改变；变化的是在某一个状态时，依据策略可能产生的具体动作，因为具体的动作是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同动作的概率。 动作值函数 Action-Value Function Q定义 $q_{\\pi}(s,a)$ 为动作值函数，表示在执行策略 $\\pi$ 时，对当前状态 s 执行某一具体动作 a 所能的到的收获的期望；或者说在遵循当前策略 $\\pi$ 时，衡量对当前状态执行动作 a 的价值大小。动作值函数一般都是与某一特定的状态相对应的。动作值函数的公式描述如下:$q_{\\pi}(s,a)= E_{\\pi}[G_t|S_t=s, A_t=a]$ 由于策略$\\pi(a|s)$是可以改变的，因此两个值函数的取值不像MRP一样是固定的，那么就能从不同的取值中找到一个最大值即最优值函数(这节课没有讲如何求解)。MDP需要解决的问题并不是每一步到底会获得多少累积reward，而是找到一个最优的解决方案。 Bellman期望方程 Bellman Expectation Equation根据这两个值函数的定义，它们之间的关系表示为 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s,a)$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’)$ 第二个式子是说当选择一个action之后，转移到不同状态下之后获取的reward之和是多少。将两个式子互相代入，可以得到如下的Bellman期望方程。 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)(R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’))$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^a\\sum_{a’\\in A}\\pi(a’|s’)q_{\\pi}(s’,a’)$ 下图解释了红色空心圆圈状态的状态价值是如何计算的，遵循的策略随机策略，即所有可能的动作有相同的几率被选择执行。和MRP类似的,我们也可以得到矩阵形式和求解 最优价值函数最优状态值函数 $v$ 指的是在从所有策略产生的状态值函数中，选取使状态 s 值最大的函数：$v_* = \\max \\limits_{\\pi} v_{\\pi}(s)$ 类似的，最优动作值函数 $q (s,a)$ 指的是从所有策略产生的动作值函数中，选取是状态动作对 &lt;$s$, $a$&gt; 价值最大的函数：$q_* (s,a) = \\max \\limits_{\\pi} q_{\\pi}(s,a)$ 最优价值函数明确了MDP的最优可能表现，当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。 最优策略当对于任何状态 s，遵循策略 $\\pi$ 的价值不小于遵循策略 $\\pi$’ 下的价值，则策略 $\\pi$ 优于策略 $\\pi$’：$\\pi \\geq \\pi^{‘} if v_{\\pi}(s)\\geq v_{\\pi^{‘}}(s) ,\\forall s $定理 对于任何MDP，下面几点成立： 存在一个最优策略，比任何其他策略更好或至少相等； 所有的最优策略有相同的最优价值函数； 所有的最优策略具有相同的动作值函数。 寻找最优策略可以通过最大化最优动作值函数来找到最优策略：对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优动作值函数，则表明我们找到了最优策略。 Bellman最优方程 Bellman Optimality Equation针对 $v$ 一个状态的最优价值等于从该状态出发采取的所有动作产生的动作价值中最大的那个动作价值：针对 $q$，在某个状态 s 下，采取某个动作的最优价值由2部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和： 组合起来，得到Bellman最优方程: 求解Bellman最优方程Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。后续会逐步讲解展开。 MDP延伸——Extensions to MDPs","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 1 Introduction","date":"2017-08-14T13:01:30.000Z","path":"2017/08/14/David_Silver_RL_1/","text":"强化学习的特点(不同于其他机器学习)： 没有监督数据、只有奖励信号 奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。 时间（序列）是一个重要因素 当前的行为影响后续接收到的数据 The RL Problem奖励 Reward 一个Reward $R_{t}$ 是信号的反馈，是一个标量 它反映 Agent 在 t 时刻做得怎么样 Agent 的工作就是最大化累计奖励 强化学习主要基于这样的”奖励假设”：所有问题解决的目标都可以被描述成最大化累积奖励。 序列决策 Sequential Decision Making 目标：选择一系列的 Action 以最大化未来的总体奖励 这些 Action 可能是一个长期的序列 奖励可能而且通常是延迟的 有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励 个体和环境 Agent &amp; Environment可以从个体和环境两方面来描述强化学习问题。在 t 时刻，Agent 可以： 做出一个行为 $A_{t}$ 有一个对于环境的观察评估 $O_{t}$ 从环境得到一个奖励信号 $R_{t}$ Environment 可以： 接收 Agent 的动作 $A_{t}$ 更新环境信息，同时使得Agent可以得到下一个观测 $O_{t+1}$ 给Agent一个奖励信号 $R_{t+1}$ 历史和状态 History &amp; State历史 历史是观测、行为、奖励的序列： $H_{t} = O_{1}, R_{1}, A_{1},…, O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}$ 状态 状态是所有能够决定将来的已有的信息，是关于历史的一个函数：$S_{t} = f(H_{t})$ 环境状态 Environment State 是环境的私有 representation 包括环境用来决定下一个观测/奖励的所有数据 通常对Agent并不完全可见，也就是Agent有时候并不知道环境状态的所有细节 即使有时候环境状态对Agent可以是完全可见的，这些信息也可能包含着一些无关信息 个体状态 Agent State 是Agent的内部representation 包括Agent可以使用的、决定未来动作的所有信息 Agent State是强化学习算法可以利用的信息 它可以是历史的一个函数： $S^{a}_{t} = f(H_{t})$ 信息状态 Information State包括历史上所有有用的信息，又称Markov状态。也就是说: 如果信息状态是可知的，那么历史可以丢弃，仅需要 t 时刻的信息状态就可以了。例如：环境状态是Markov的，因为环境状态是环境包含了环境决定下一个观测/奖励的所有信息 同样，（完整的）历史 $H_{t}$ 也是Markov的。 完全可观测的环境 Fully Observable Environments Agent能够直接观测到环境状态: $O_{t} = S^{a}_{t} = S^{e}_{t}$ 正式地说，这种问题是一个马尔可夫决策过程（Markov Decision Process， MDP） 部分可观测的环境 Partially Observable Environments Agent 间接观测环境。举了几个例子： 一个可拍照的机器人Agent对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一； 一个交易员只能看到当前的交易价格； 一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。 在这种条件下：个体状态 ≠ 环境状态 正式地说，这种问题是一个部分可观测马尔可夫决策过程 (POMDP)。Agent 必须构建它自己的状态 representation $S^{a}_{t}$，比如： 记住完整的历史： $S^{a}_{t} = H_{t}$ 这种方法比较原始、幼稚。还有其他办法，例如 ： Beliefs of environment state：此时虽然 Agent 不知道环境状态到底是什么样，但Agent可以利用已有经验（数据），用各种 Agent 已知状态的概率分布作为当前时刻的 Agent 状态的呈现：$S^{a}_{t} = (P[S^e_t=s^1],…,P[S^e_t=s^n])$ Recurrent neural network：不需要知道概率，只根据当前的Agent状态以及当前时刻Agent的观测，送入循环神经网络(RNN)中得到一个当前Agent状态的呈现：$S^{a}_{t} = \\sigma(S^e_{t-1}W_s + O_tW_o)$ Agent的主要组成部分强化学习中的Agent可以由以下三个组成部分中的一个或多个组成： 1. 策略 Policy策略是决定Agent行为的机制。是从状态到行为的一个映射，可以是确定性的，也可以是不确定性的。 确定的 Policy : $a=\\pi(s)$ 随机 Policy : $\\pi(a|s) = P[A_t=a|S_t=s]$ 2. 价值函数 Value Function 是一个对未来奖励的预测 用来评价当前状态的好坏程度 当面对两个不同的状态时，Agent可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。同时，一个价值函数是基于某一个特定策略的，不同的策略下同一状态的价值并不相同。某一策略下的价值函数用下式表示：$V_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+…|S_t=s]$ 3. 模型 ModelAgent对环境的一个建模，它体现了Agent是如何思考环境运行机制的（how the agent think what the environment was.），Agent希望模型能模拟环境与Agent的交互机制。 模型至少要解决两个问题：一是状态转化概率，即预测下一个可能状态发生的概率：$P^a_{s s^{‘}} = P[S_{t+1}=s^{‘}|S_t=s,A_t=a]$ 另一项工作是预测可能获得的即时奖励：$R^a_{s} = E[R_{t+1}|S_t=s,A_t=a]$ 模型并不是构建一个Agent所必需的，很多强化学习算法中Agent并不试图（依赖）构建一个模型。 注：模型仅针对Agent而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定Agent下一个状态和所得的即时奖励。 Agent的分类解决强化学习问题，Agent可以有多种工具组合，比如通过建立对状态的价值的估计来解决问题，或者通过直接建立对策略的估计来解决问题。这些都是Agent可以使用的工具箱里的工具。因此，根据Agent内包含的“工具”进行分类，可以把Agent分为如下三类： 仅基于价值函数的 Value Based：在这样的Agent中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。 仅直接基于策略的 Policy Based：这样的Agent中行为直接由策略函数产生，Agent并不维护一个对各状态价值的估计函数。 演员-评判家形式 Actor-Critic：Agent既有价值函数、也有策略函数。两者相互结合解决问题。 此外，根据Agent在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类： Model Free:这类Agent并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。 Model Based：Agent尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。 学习和规划 Learning &amp; Planning学习： 环境初始时是未知的 Agent不知道环境如何工作 Agent通过与环境进行交互，逐渐改善其行为策略。 规划: 环境如何工作对于Agent是已知或近似已知的 Agent并不与环境发生实际的交互，而是利用其构建的模型进行计算 在此基础上改善其行为策略。 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。 探索和利用 Exploration &amp; Exploitation强化学习是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。 在这样的学习过程中，就会有一个在Exploration和Exploitation之间的权衡，前者是说会放弃一些已知的reward信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么action让reward比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让reward更大，即Exploration希望能够探索更多关于environment的信息。而后者是指根据已知的信息最大化reward。例如，在选择一个餐馆时，Exploitation会选择你最喜欢的餐馆，而Exploration会尝试选择一个新的餐馆。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"从Fictitious Play 到 NFSP","date":"2017-07-27T13:13:00.000Z","path":"2017/07/27/从Fictitious Play 到 NFSP/","text":"博弈论Normal-form game在博弈论中，Normal-form game是对game的一种描述。Normal-form game通过矩阵来表示game。下图就是一个payoff矩阵：Normal-form game是一种静态模型，这个模型假设每个player仅选择一次action或策略。Normal-form game适用于描述不需要考虑博弈进程的完全信息静态(Complete information static)博弈。 静态博弈players同时进行决策 动态博弈players的决策顺序有先后 完全信息(Complete information)在完全信息博弈中，players的游戏结构和players的payoff矩阵是众所周知的，但players可能看不到其他players所做的所有动作。 完美信息(Perfect information)在完美信息博弈中，每个player可以观测到其他players的动作，但可能缺乏关于他人的payoff或游戏结构的信息。 Extensive-form gameExtensive-form game是一种动态模型。通过使用树这种图形来表示game。不仅给出了博弈结果，还描述了博弈过程，如给出博弈中参与人的行动顺序，以及决策环境和行动空间等。完美信息下的Extensive-form game:If player 1 plays D, player 2 will play U’ to maximise his payoff and so player 1 will only receive 1. However, if player 1 plays U, player 2 maximises his payoff by playing D’ and player 1 receives 2. Player 1 prefers 2 to 1 and so will play U and player 2 will play D’ 非完美信息下的Extensive-form game: 信息集 是一组决策节点： 集合中的每个节点都只属于一个player； 当player到达信息集时，player不能区分信息集中的节点。即，如果信息集包含多个节点，则该集合所属的player不知道集合中的哪个节点已经到达。 下图的虚线连接的两个节点2代表player2的一个信息集，player2不知道player1采取的是U还是D。补充：在扑克游戏中，player不知道的是对方的手牌信息而不是这个例子中的动作。player 2 cannot observe player 1’s move. Player 1 would like to fool player 2 into thinking he has played U when he has actually played D so that player 2 will play D’ and player 1 will receive 3. ref : https://en.wikipedia.org/wiki/Extensive-form_game Extensive-form game和Normal-form game的转换下图是Extensive-form game的一个例子：player1有三个信息集，每个信息集里面有两种行动，所以player1有8种策略；player2有两个信息集，每个信息集里面有两种行动，所以player2有4种策略。所以转换成Normal-form game是一个横8纵4的矩阵. FSP in Extensive-Form GamesExtensive-Form 定义扩充 ChanceChance被认为是一个特别的player，遵循一个固定的随机策略，决定了chance节点进入每个分支的概率分布。（如扑克游戏中的发牌）。 Information State我们假设游戏具有perfect recall，即每个player当前的信息状态u隐含了之前的信息状态和行动序列: Behavioural Strategy在给定Information State时，动作action的概率分布。Behavioural Strategy 输出player i在信息状态u下，动作的概率分布。Normal-Form 定义扩充 Pure Strategyplayer在可能遇到的所有情况下都有一个确定的动作action。(payoff矩阵中的每一行和列都是一个Pure Strategy) Mixed StrategyMixed Strateg是Player i的Pure Strategy的概率分布。 Fictitious Play在normal form下：当前策略 = 现有策略和bestResponse的加权平均。 Realization-equivalence也就是说，Extensive-Form下的Behavioural Strategy和Normal-form下的Mixed Strategy存在一种等价。Extensive-Form也可以使用Normal-form下的Fictitious Play来求纳什均衡解。 Extensive-Form Fictitious Play 代表到信息状态u的sequence = sequence 中每个节点选择每个action的概率乘积 下图是对Lemma 6的一个计算的验证。 我们通过Theorem7更新Behavioural Strategy，可以收敛到纳什均衡。 具体算法XFP： 计算bestResponse 更新策略，使用Theorem7 重复上面的过程 Fictitious Self-PlayXFP会有维度灾难。generalised weakened fictitious play只需要近似bestResponse，甚至允许更新中的一些扰动。 Fictitious Self-Play： 使用强化学习去计算bestResponse 使用监督学习更新策略 样本的采集 GENERATEDAT()方法需要在理解一下。 Neural Fictitious Self-Play强化学习和监督学习都使用神经网络去学习出一个函数。 FSP中样本采集的混合策略 ，在NSFP中是函数，不能简单相加。转换成 ，使用概率完成上面式子中的加操作。 REF： Topics in Algorithmic Game Theory February 8, 2010 Lecturer: Constantinos Daskalakis Lecture 2 Fictitious Self-Play in Extensive-Form Games Deep Reinforcement Learning from Self-play in imperfect-information games","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gyh75520.github.io/tags/博弈论/"},{"name":"ML","slug":"ML","permalink":"https://gyh75520.github.io/tags/ML/"}]},{"title":"opencv3 --python3 mac下配置","date":"2017-07-22T11:38:00.000Z","path":"2017/07/22/opencv3 --python3 mac配置/","text":"安装Anaconda把Anaconda安装的python版本设置为默认启动版本修改.bash_profile文件：123456789101112# Setting PATH for Python 2.7 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;&quot; export PATH # Setting PATH for Python 3.4 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.4/bin:$&#123;PATH&#125;&quot; export PATH # added by howard export PATH=&quot;/usr/local/anaconda3/bin:$PATH&quot; PATH替换成自己的路径 根据该脚本，先会去找 /usr/local/anaconda3/bin ，发现有，就为当前路径下的解释器环境，并执行。—— 所以，想设置python的版本，直接把你想添加的路径export上去，并放在后面。ref: http://blog.csdn.net/u010692239/article/details/52701626 安装opencv1brew install opencv3 --with-python3 安装好后，在最后它会提示你如果想要Python也能调用opencv接口的话，需要执行下面命令：1234If you need Python to find bindings for this keg-only formula, run:```bashecho /usr/local/opt/opencv3/lib/python2.7/site-packages &gt;&gt; /usr/local/lib/python2.7/site-packages/opencv3.pth echo打印输出，&gt;&gt;重定向，执行完这句，可以在/usr/local/lib/python2.7/site-packages/目录下得到一个文件opencv3.pth。但是我们来看看它所放置的目录，这个目录是系统自带的Python目录，而我们使用的Anaconda里的Python，所以你需要将其重定向输出的路径改到Anaconda中Python目录下，比如我的：1echo /usr/local/opt/opencv3/lib/python3.6/site-packages &gt;&gt; /usr/local/anaconda3/lib/python3.6/site-packages/opencv3.pth ref: http://blog.csdn.net/willard_yuan/article/details/46721831 安装完成之后你一定会迫不及待的去测试一下：123pythonimport cv2RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa 如果出现上述错误，证明你numpy的版本不对，升级版本：1pip install numpy --upgrade","tags":[{"name":"opencv3","slug":"opencv3","permalink":"https://gyh75520.github.io/tags/opencv3/"},{"name":"mac","slug":"mac","permalink":"https://gyh75520.github.io/tags/mac/"}]},{"title":"HEXO+Github搭建博客","date":"2016-10-17T15:33:00.000Z","path":"2016/10/17/first/","text":"hexo是一款基于Node.js的静态博客框架。 之前是想着写博客，一方面是给自己做笔记，可以提升自己的写作、总结能力，这里记录一下linux下面Hexo搭建的步骤。 配置环境 安装Node.js 安装Git Github新建仓库和连接新建repository(仓库)登陆Github账号后，点击右上角的“+”号按钮，选择“New repository” 在Create a new repository界面填写仓库名必须为【your_user_name.github.io】固定写法 填写完成点Create repository创建完成 生成SSH Keys：我们如何让本地git项目与远程的github建立联系？这时候就要用到SSH Keys 使用ssh-keygen命令生成密钥对 1ssh-keygen -t rsa -C\"这里是你申请Github账号时的邮箱\" 然后系统会要你输入密码：（我们输入的密码会在你提交项目的时候使用） 12Enter passphrase (emptyforno passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; （终端提示生成的文件路径）找到你生成的密钥找到id_rsa.pub用终端进入编辑，复制密钥。 添加你的SSH Key到ssh-agent登陆Github,点击右侧用户按钮，选择Settings 点击 Add SSH key 按钮，将复制的密钥粘贴到 Key 栏 测试能不能链接成功1ssh -T git@github.com 执行结果 Permanently addedtheRSA host keyforIP address ‘192.30.252.130’tothelistofknown hosts.Are you sure you wanttocontinueconnecting (yes/no)?&lt;输入yes&gt;Hi username! You’ve successfully authenticated,butGitHubdoesnot 现在你已经可以通过SSH链接到Github了 正式安装HexoNode和Git都安装好后,首先创建一个文件夹,如blog,用户存放hexo的配置文件,然后进入blog里安装Hexo。我安装的是hexo 3.0以上的版本。 执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 初始化然后，执行init命令初始化hexo,命令： 1hexo init 好啦，至此，全部安装工作已经完成！blog就是你的博客根目录，所有的操作都在里面进行。 生成静态页面 1hexo generate（hexo g也可以） 本地启动 启动本地服务，进行文章预览调试，命令： 1hexo server 浏览器输入http://localhost:4000 然后建立关联，blog文件夹下有： _config.yml node_modules public source db.json package.json scaffolds themes 现在我们需要修改_config.yml文件，翻到最下面，改成像我这样：1234567deploy: type: git repo: https://github.com/**gyh75520**/**gyh75520**.github.io.git branch: master 加粗的部分替换成前面的配置的your_user_name 然后执行命令： 1npm install hexo-deployer-git --save 然后，执行配置命令： 1hexo deploy 到这里项目已经部署到了github pages上 访问https://your_user_name.github.io 查看自己的博客吧O(∩_∩)O 备注：没有权限的话记得在命令前加上sudo Mac篇在mac配置，执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 报错:1npm ERR! 解决：把官方的源替换成淘宝的源，替换的方法12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install -g hexo-cli Hexo下mathjax的转义问题我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax,可以解析网页上的数学公式，与hexo的结合也很简单，可以手动加入js，或者直接使用hexo-math插件.大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候: — 的转义，在markdown中，_是斜体，但是在latex中，却有下标的意思，就会出现问题。 \\\\\\的换行，在markdown中，\\\\\\会被转义为\\\\,这样也会影响影响mathjax对公式中的\\\\\\进行渲染 解决办法：参考 https://segmentfault.com/a/1190000007261752","tags":[{"name":"HEXO","slug":"HEXO","permalink":"https://gyh75520.github.io/tags/HEXO/"}]}]