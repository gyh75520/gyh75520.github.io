[{"title":"UNREAL（UNsupervised REinforcement and Auxiliary Learning）算法","date":"2018-08-30T14:01:30.000Z","path":"2018/08/30/Unreal/","text":"作者通过添加辅助任务增强了A3C（Asynchronous Actor Critic）算法。这些辅助任务共享网络参数，但是它们的价值函数是通过 n-step 的 off-policy 的 Q-Learning 来学习的。辅助任务只用于学习更好的表示，而不直接影响主任务的任务control。这种改进被称为UNREAL（Unsupervised Reinforcement and Auxiliary Learning），在性能和训练效率方面优于A3C。 ideaA3C 算法充分使用了 Actor-Critic 框架，是一套完善的算法，因此，我们很难通过改变算法框架的方式来对算法做出改进。UNREAL 算法在 A3C 算法的基础上，另辟蹊径，通过在训练 A3C 的同时，训练多个辅助任务来改进算法。UNREAL 算法的基本思想来源于我们人类的学习方式。人要完成一个任务，往往通过完成其他多种辅助任务来实现。比如说我们要收集邮票，可以自己去买，也可以让朋友帮忙获取，或者和其他人交换的方式得到。UNREAL 算法通过设置多个辅助任务，同时训练同一个 A3C 网络，从而加快学习的速度，并进一步提升性能。 UNREAL 算法本质上是通过训练多个面向同一个最终目标的任务来提升行动网络的表达能力和水平，符合人类的学习方式。值得注意的是，UNREAL 虽然增加了训练任务，但并没有通过其他途径获取别的样本，是在保持原有样本数据不变的情况下对算法进行提升，这使得 UNREAL 算法被认为是一种无监督学习的方法。基于 UNREAL 算法的思想，可以根据不同任务的特点针对性地设计辅助任务，来改进算法。 网络结构 更详细的网络结构（来自 https://github.com/miyosuda/unreal） ： AUXILIARY CONTROL TASKS包括像素控制和隐藏层激活控制。 像素控制（\bPixel Control）像素控制是指控制输入图像的变化，使得图像的变化最大。因为图像变化大往往说明Agent在执行重要的环节，通过控制图像的变化能够改善动作的选择。 如何定义\b该任务的 pseudo-reward：将网络输入的 84x84 RGB 图片裁剪成 80x80 的图片，将裁剪的图片以 4x4 cell 为单位分成 20x20 个 gird，每个 cell 的 reward 通过计算每个pixel 和 channels 的像素差的平均值得到。网络最后的输出是 $N_{act}$ x20x20 的 \b$Q^{aux}$。 具体的网络结构参照图 特征控制（Feature Control）隐藏层激活控制则是控制隐藏层神经元的激活数量，目的是使其激活量越多越好。这类似于人类大脑细胞的开发，神经元使用得越多，可能越聪明，也因此能够做出更好的选择。 类似于 Pixel Control 对 pseudo-reward 的定义，我们计算隐藏层的神经元激活数量作为reward。网络最后的输出也是 $N_{act}$ x20x20 的 $Q^{aux}$。 为了更好的理解辅助控制任务，我们和另外两种很自然的辅助任务的方法做比较。 第一个baseline是pixel reconstruction loss，类似于DSR中对 $\\phi(s_i)$进行反卷积，能够使得网络能够更好的区分不同的状态。 第二个baseline是input change prediction loss，这个可以被看作是预测\b立即的reward，reward的计算类似Pixel Control。（ pixel control 是control任务,它预测的是Q值）。 从上图可以看出，pixel reconstruction 可以加速初始的学习，但是会损坏最后的得分。因为它过于专注于重建视觉输入中不相关的部分而没有专注于和奖励有关的部分。 AUXILIARY REWARD TASKS因为在很多场景下，回馈 r 并不是每时每刻都能获取的（比如在 Labyrinth 中吃到苹果才能得1分），所以让神经网络能够预测回馈值会使其具有更好的表达能力。为了学习到最大化reward的policy，Agent 需要能识别出那些可能会产生高 reward 的状态。对 rewarding 状态有良好表示的 Agent能够学习到更好的价值函数，这也意味着Agent能够更容易和更好的学习到策略。 与学习价值函数和学习策略不同，Reward Prediction 任务仅仅只是用来优化Agent 的 features，因此加入 Reward Prediction 不会对原有的价值函数和策略产生偏差。 在 UNREAL 算法中，使用历史连续多帧的图像输入来预测下一步的 reward 作为训练目标，这是一个三分类任务，预测值为 +，0，- 的概率。至关重要的是，由于奖励往往是稀疏的，我们使用 skewed 技术使得 $P(r != 0) = P(r = 0)= 0.5$。能这么做的原因，我猜测是因为这只是一个监督学习。我们只需要将 replay memory 分割成两个集合，分别是reward = 0 的样本和 reward ！= 0 的样本。 另一个细节是，这里并没有用 LSTM。作者也解释了原因，大概是说这个 auxiliary task，reward prediction 的目的是关注 immediate reward, rather than long-term returns。 Value Function Replay相当于从replay buffer中重新采样进行 V 值函数回归。这一部分相当于将A3C输出的V值网络训练的更好，V值网络更好，策略网络也会更好。 UNREAL结合上述的辅助任务，得到UNREAL算法，Loss函数如下：$$L_{UNREAL}(\\theta)= L_{A3C}+\\lambda_{VR} L_{VR} + \\lambda_{PC} \\sum_cL_{Q}^{(c)}+\\lambda_{RP} L_{RP}$$ 其中 VR = Value Function Replay, PC = Pixel Control, RP = Reward Prediction。在实际的训练中，Loss被拆分成单独的部分训练。 实验实验环境Atari 和 3D迷宫游戏 Labyrinth \b实验设置 \bAgent 训练使用的是 n-step Returns，\bn = 20 Replay \bBuffer 的大小 = 2000 辅助控制任务使用 off-polcy RL 算法（例如，n-step Q-Learning），这样就可以使用经验回放 结果Labyrinth：38% -&gt; 83% human-normalized score.Significantly faster learning, 11x across all levels. Atari：表现更好，更robust。 REF: Jaderberg M, Mnih V, Czarnecki W M, et al. Reinforcement Learning with Unsupervised Auxiliary Tasks[J]. 2016.","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"深度后续强化学习 Deep Successor Reinforcement Learning","date":"2018-05-30T14:01:30.000Z","path":"2018/05/30/Deep_Successor_RL/","text":"一般地，在只给定原始输入观察和奖赏值的情况下，通过基于模型（model-based）或者模型无关 （model-free）的DRL算法可以学习到鲁棒的值函数。后续状态表示法 （Successor Representation， SR）为学习值函数提供了第 3 种选择。 model-based 和 model-free（图片来自论文：The successor representation in human reinforcement learning） MB: Agent 学习环境模型，利用这个模型，我们就可以进行动态规划。如果状态和动作空间巨大，对计算力就会有很大的要求。 MF: Agent 不需要对环境进行建模，将长期动作值直接存储起来，一个经典的例子就是Q-Learning ，对动作值函数 Q 存储起来。因缓存了值函数，所以计算就很高效。 SR: 结合了 MF 的高效性和 MB 的灵活性。对突出奖赏(distal reward)的变化十分敏感。有能力分解出更有价值的子目标。 Successor RepresentationSR 将值函数分 解为两个部分： 后续状态映射图（successor map） 奖赏预测（reward predictor） 后续状态映射图表示在给定当前状态下到达未来某一状态占有率的期望。奖赏预测表示从状态到奖赏 值的映射。在 SR 中，这两个部分以内积的形式构成值函数． $$ Q^\\pi(s,a) = \\mathbb{E}[\\sum^{\\infty}_{t=0}\\gamma^tR(s_t)|s_0 = a,a_0 =a]$$$$ M(s,s’,a)=\\mathbb{E}[\\sum^{\\infty}_{t=0}\\gamma^t1[s_t=s’]|s_0=s,a_0=a]$$ 其中 $1[.]$当参数为true时=1$$ Q^{\\pi} = \\sum_{s’ \\in S} M(s,s’,a)R(s’) $$我们可以看到，Q 值的计算是每个时间step下带有折扣因子$\\gamma$的\breward之和。而SR的计算方式是先计算一个状态在未来占有率的期望，乘上 $R(s)$ 就得了该状态在未来的reward，然后将所有状态的 reward\b 累加起来。 Deep Successor Reinforcement Learning理解了SR ， 我们就希望能够将 SR 和深度学习模型结合起来。 很自然的想法，我们可以构造 $M(s,s’,a)$ 网络，输入是 s 和 s‘，输出是各个动作的 M 值（假设动作空间是离散的）。同样的，$R(s)$ 网络的输入是 s ，输出是该状态 s 下的 reward。 问题就是大部分环境下，状态\b是连续的，很明显，遍历所有状态的 M和 R 值然后计算 Q 值是不现实的。 \b本文作者设计的结构如下： \b\b第一部分，状态 $s_t$ 通过卷积层（参数为 $\\theta$ )，得到高阶特征，然后平铺成 512 维度特征 $\\phi(s_t)$。然后使用一个简单的线性回归得到奖赏预测 $R(s_t) \\approx \\phi(s_t) \\cdot w$ 。w 表示权值向量。 第二部分 ，基于 $\\phi(s_t)$ 使用一个参数为 $\\alpha$ 的深度神经网络 $u_\\alpha $ 来近似表示 \b$m(s_t,a) \\approx u_\\alpha(\\phi(s_t,),a)$。 为了更好的理解图中的 $m_{s,a}$,$Q^{\\pi} = \\sum_{s’ \\in S} M(s,s’,a)R(s’)$ $Q^{\\pi} = \\sum_{s’ \\in S} M(s,s’,a)\\phi(s’) * w$ 因为 w 一旦训练好之后就是固定的，所以$Q^{\\pi} = (\\sum_{s’ \\in S} M(s,s’,a)\\phi(s’)) * w$我们只要构造一个网络 \b$m(s,a) \\approx \\sum_{s’ \\in S} M(s,s’,a)\\phi(s’)$$Q^{\\pi} = m(s,a) \\cdot w$ 这样就可以免去对所有状态的\b遍历！ 因为参数 $\\theta$ 训练后用来获取 $\\phi(s)$, $\\phi(s_t)$ 同时是第一部分和第二部分的输入，这就要求$\\phi(s)$ ：1）能够准确预测 状态 s 的 reward2）能够区分不同的状态 第一点只要最小化损失函数 $L^r_t(w,\\theta) = (R(s_t)-\\phi_{s_t}w)^2$，第二点我们需要使用一个深度卷积自动编码器（反卷基）重建图像，定义一个L2损失函数：$L^a_t(\\theta’,\\theta) = (g_{\\tilde{\\theta}}(\\phi_{s_t})-s_t)^2$ 因为 $Q(s_t,a) = R(s_t) + \\gamma max_{a’}Q(s_{t+1,a’})$ $m_{s_t,a}\\cdot w = \\phi_{s_t} \\cdot w + \\gamma m_{s_{t+1,a’}}\\cdot w$ $m_{s_t,a} = \\phi_{s_t} + \\gamma m_{s_{t+1,a’}}$ 其中 $a’ = argmax_am_{s_{t+1},a} \\cdot w$ 所以successor map 部分的 Loss 为:$L^m_t(w,\\theta) = \\mathbb{E}[(\\phi_{s_t}+\\gamma u_{\\alpha_{prev}}(\\phi_{s_t+1},a’) - u_{\\alpha}(\\phi_{s_t},a))^2]$ 我们先最小化 $L^r_t(w,\\theta)+L^a_t(\\theta’,\\theta)$ 得到最优的\b参数$(\\theta^{opt} ,w^{opt} )$,然后固定这两个参数我们\b最小化$L^m_t(w,\\theta)$,得到最优的 $\\alpha^{opt} $。 算法 这部分类似SARSA的过程。 自动子目标提取 给定一个随机策略，训练\b\b\b DSR 直到收敛，我们可以获得大量的序列 $\\tau = {m_{s_1,a_1},m_{s_2,a_2}…,m{s_n,a_n}}$。通过归一化割(Nomalized cut)算法 得到一些 subgoal。 这种方法的一个固有局限性是，由于随机策略，子目标候选常常非常嘈杂。此外，子目标提取算法应该是非参数的而且处理灵活数量的子目标。 实验在两个游戏环境和DQN的比较：\b为了证明SR对突出奖赏(distal reward)的变化十分敏感。 总结第一篇将SR\b和\b深度学习模型结合。DSR可以自动提取子目标，这就可以与分层强化学习相结合。 REF: Kulkarni T D, Saeedi A, Gautam S, et al. Deep Successor Reinforcement Learning[J]. 2016.","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"事后经验回放 Hindsight Experience Reply","date":"2018-03-22T14:01:30.000Z","path":"2018/03/22/Hindsight_Experience_Reply/","text":"提出一种新的经验回放方法，能够在稀疏且binary reward 环境中训练 RL 算法。 idea人类在学习的时候，很多情况下不能完全也不需要完全达到自己特定的目标，才能学到特定的经验和技术。人类在学习的时侯，可能会尝试不同的手段和方法来做一件事，虽然可能这个方法在特定的任务上T不奏效，但这样的方法可能完成了其他的任务T’，当你下次需要做个任务T’时，你可以用这些经验来完成。比如在一个射击靶子游戏中，靶子随机出现某个位置，射中reward = 0 否则为 -1 。你可能很多都枪射歪了，这对强化学习来说，这些样本学不到任何有用的经验。但是如果把射歪的位置看成靶子的位置，这对强化学习来说就是一个有用的样本。将射歪的位置看成靶子的位置，相当于设置了一个新的 Goal。 所以基于上面的思路，如果我们的目的是做一类比较接近的goal的话，或者我们能构造出与当前goal比较接近的一系列goal的话，我们就有可能利用另外的goal来衡量policy在环境中的trajectory 的好坏，虽然在大部分的goal中这个轨迹可能是比较差的，但是如果我们的goal是要达到 s_n 的话，那么这个policy其实好的。然后利用这个trajectory来进行学习，这里值得注意一下，我个人的理解是：对于这些goal需要具有一定的联系（内在的相似性），这样这个trajectory训练出来的效果才有可能对于完成另外goal有帮助。 所以既然感觉有点浪费，就会想要利用起来，这部分也就是HER做的事情：如果我们能够知道r(s, a, g)的话，那么对于上面采样出来的 $\\tau$ 中的 (s, a, r(a, s, g), s’, g) ，我们可以选择不同的goal，让这里面的reward变成1，就是意味着：这个transition tuples能够有效地帮助这个goal进行学习。那么replay buffer中reward为1的transition tuples数目就得到了一定的提升，可能就能够有效地帮助agent学习。 下面是HER的算法，简单地解释一下就是：利用当前policy在环境中交互获得trajectory $\\tau$ ，然后将 (s, a, r(a, s, g), s’, g) 存储在replay buffer中，然后再挑选一些其他的goal对这个trajectory $\\tau$ 中的g和r做修改，然后存储在replay buffer中，之后就是普通的基于replay buffer算法中常见的从buffer中sample，然后训练等过程中。 算法下面是HER的算法，简单地解释一下就是：利用当前policy在环境中交互获得 trajectory $\\tau$ ，然后将 (s, a, r(a, s, g), s’, g) 存储在 replay buffer 中，然后再挑选一些其他的 goal 对这个 trajectory $\\tau$ 中的 g 和 r 做修改，然后存储在r eplay buffer 中，之后就是普通的基于replay buffer 算法中常见的从 buffer 中 sample，然后训练等过程中。 那么关于如果挑选其他的goal就是一项很玄学的地方了，在论文里面提出了几种不同的方法： final — goal corresponding to the final state in each episodefuture — replay with k random states which come from the same episode as the transition being replayed and were observed after itepisode — replay with k random states coming from the same episode as the transition being replayedrandom — replay with k random states encountered so far in the whole training procedure 实验实验环境 这里有三种任务： Pushing. 把物体推到指定的位置 Sliding. 推动物体，使它滑动到某个位置 Pick-and-place. 拿起物体，移动到空中的某个位置 在这个环境中： Reward：在没有到到达goal时，都是-1，到达goal时候为0 Goal：为在空间中随机生成的位置（所以我感觉这也是有效的一点） Observations：gripper（机器手）在这个空间中的绝对位置，需要推动物体object和goal相对gripper的相对位置 一些局限HER目前看上去局限很多（当然也就是改进的地方）。比如这里就直接假设reward和goal可以直接控制的，但是很多情况下并不是，可能我们就是要实现固定的几个goal，而且不知道这里面的reward，同样goal之间的关系可能不是特别紧密，那么HER该怎么用呢？（基本假设都出现了问题） 另外就是这里实验的设计，goal变了，其实导致了 （s，a，r, s’, g） 中 s 的改变(s 和 goal 进行拼接)，这个s里面是会体现goal的，但是很多时候，我们无法直接修改s，比如玩video game。 最后，HER 是和 Off-Policy 的 RL 算法进行结合，原因是：On-Policy 中 Q 值的更新用的 s‘ 是确定的，我们选择其他的goal的时候，无法及时更新到 s‘||新goal 的 Q 值。Off-Policy 中的 max 操作 可以保证我们更新到新的 Q 值。 REF: Andrychowicz M, Wolski F, Ray A, et al. Hindsight Experience Replay[J]. 2017.","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 5 模型无关控制 Model-Free Control","date":"2018-03-11T09:01:30.000Z","path":"2018/03/11/David_Silver_RL_5/","text":"简介 Introduction上一讲主要讲解了在模型未知的情况下如何进行预测。所谓的预测就是评估一个给定的策略，也就是确定一给定策略下的状态（或状态行为对）的价值函数。这一讲的内容主要是在模型未知的条件下如何优化价值函数，这一过程也称作模型无关的控制。 现实中有很多此类的例子，比如控制一个大厦内的多个电梯使得效率最高；控制直升机的特技飞行，机器人足球世界杯上控制机器人球员，围棋游戏等等。所有的这些问题要么我们对其模型运行机制未知，但是我们可以去经历、去试；要么是虽然问题模型是已知的，但问题的规模太大以至于计算机无法高效的计算，除非使用采样的办法。本节的内容就专注于解决这些问题。根据优化控制过程中是否利用已有或他人的经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类： 一类是遵循策略学习（On-policy Learning）: 其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。由于要优化的策略就是当前遵循的策略，这里姑且将其翻译为“遵循策略”。 另一类是脱离策略学习（Off-policy Learning）: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类的策略等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。这种学习方式类似于“站在别人的肩膀上可以看得更远”。由于这些策略是已有的策略，这里姑且翻译为“脱离策略”。 On-Policy Monte-Carlo Control在本节中我们使用的主要思想仍然是动态规划的思想。先来回顾下动态规划是如何进行策略迭代的。 通用策略迭代（回顾） 通用策略迭代的核心是在两个交替的过程之间进行策略优化。一个过程是策略评估，另一个是改善策略。如上图的三角形区域所示，从一个策略π和一个价值函数Ｖ开始，每一次箭头向上代表着利用当前策略进行价值函数的更新，每一次箭头向下代表着根据更新的价值函数贪婪地选择新的策略，说它是贪婪的，是因为每次都采取转移到可能的、状态函数最高的新状态的行为。最终将收敛至最优策略和最优价值函数。 注意使用动态规划算法来改善策略是需要知道某一状态的所有后续状态及状态间转移概率：$\\pi ^{‘}(s)= \\arg\\max_{a\\in A} R_s^a + P_{ss^{‘}}^a V(s^{‘})$ 不基于模型控制的两个条件那么这种方法是否适用于模型未知的蒙特卡洛学习呢？答案是否定的，这其中至少存在两个问题。其一是在模型未知的条件下无法知道当前状态的所有后续状态，进而无法确定在当前状态下采取怎样的行为更合适。解决这一问题的方法是，使用状态行为对下的价值Q(s,a)来代替状态价值 ： $ \\pi ^{‘}(s)= \\arg\\max_{a\\in A} Q(s,a)$ 这样做的目的是可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么什么样的行为价值最大即可。具体是这样：我们从一个初始的 Q 和策略 $\\pi$ 开始，先根据这个策略更新每一个状态行为对的 Q 值，s 随后基于更新的 Q 确定改善的贪婪算法。 即使这样，至少还存在一个问题，即当我们每次都使用贪婪算法来改善策略的时候，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略，我们需要不时的尝试一些新的行为，这就是探索（Exploration）。 $\\epsilon$-贪婪探索 的目标使得某一状态下所有可能的行为都有一定非零几率被选中执行，也就保证了持续的探索，$1-\\epsilon$ 的概率下选择当前认为最好的行为，而 $\\epsilon$ 的概率在所有可能的行为中选择（也包括那个当前最好的行为）。数学表达式如下： 定理： 使用Ɛ-贪婪探索策略，对于任意一个给定的策略\\pi，我们在评估这个策略的同时也总在改善它。 证明： 注：在证明上述定理过程中使用的不等式是在经过合理、精心设计的。 解决了上述两个问题，我们最终看到蒙特卡洛控制的全貌：使用Ｑ函数进行策略评估，使用Ɛ-贪婪探索来改善策略。该方法最终可以收敛至最优策略。如下图所示： 图中每一个向上或向下的箭头都对应着多个 Episode 。也就是说我们一般在经历了多个 Episode 之后才进行依次Ｑ函数更新或策略改善。实际上我们也可以在每经历一个 Episode 之后就更新Ｑ函数或改善策略。但不管使用那种方式，在Ɛ-贪婪探索算下我们始终只能得到基于某一策略下的近似Ｑ函数，且该算法没没有一个终止条件，因为它一直在进行探索。因此我们必须关注以下两个方面：一方面我们不想丢掉任何更好信息和状态，另一方面随着我们策略的改善我们最终希望能终止于某一个最优策略，因为事实上最优策略不应该包括一些随机行为选择。为此引入了另一个理论概念：GLIE。 GLIEGLIE(Greedy in the Limit with Infinite Exploration)，直白的说是在有限的时间内进行无限可能的探索。具体表现为：所有已经经历的状态行为对（state-action pair）会被无限次探索；另外随着探索的无限延伸，贪婪算法中Ɛ值趋向于０。例如如果我们取 $\\epsilon = 1/k$（ k 为探索的 Episode 数目），那么该Ɛ贪婪蒙特卡洛控制就具备 GLIE 特性。 基于GLIE的蒙特卡洛控制流程如下： 对于给定策略 $\\pi$ ，采样第k个 Episode:{ $S_{1}, A_{1}, R_{2}, …, S_{T} $} ～ $\\pi$ 对于该Episode里出现的每一个状态行为对S_t和A_t,更其计数和Ｑ函数：$N(S_t, A_t) \\leftarrow N(S_t, A_t) + 1 , Q(S_t, A_t) \\leftarrow + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))$ 基于新的Ｑ函数改善以如下方式改善策略：$\\epsilon \\leftarrow 1/k , \\pi \\leftarrow \\epsilon-greedy(Q)$ 定理：GLIE 蒙特卡洛控制能收敛至最优的状态行为价值函数。 On-Policy Temporal-Difference Control上一讲提到TD相比MC有很多优点：低变异性，可以在线实时学习，可以学习不完整Episode等。因此很自然想到是否可以在控制问题上使用TD学习而不是MC学习？答案是肯定的，这就是下文要讲解的SARSA。 SARSASARSA 的名称来源于下图所示的序列描述：针对一个状态S，以及一个特定的行为A，进而产生一个状态行为对(S,A)，与环境交互，环境收到个体的行为后会告诉个体即时奖励R以及后续进入的状态S’；接下来个体遵循现有策略产生一个行为A’，根据当前的状态行为价值函数得到后一个状态行为对(S’,A’)的价值（Q），利用这个Q值更新前一个状态行为对(S,A)的价值。 更直观的解释是这样：一个Agent处在某一个状态S，在这个状态下它可尝试各种不同的行为，当遵循某一策略时，会根据当前策略选择一个行为A，个体实际执行这个行为，与环境发生实际交互，环境会根据其行为给出即时奖励R，并且进入下一个状态S’，在这个后续状态S’，再次遵循当前策略，产生一个行为A’，此时，个体并不执行该行为，而是通过自身当前的状态行为价值函数得到该S’A’状态行为对的价值，利用该价值同时结合个体S状态下采取行为A所获得的即时奖励来更新个体在S状态下采取A行为的（状态）行为价值。 与蒙特卡洛控制不同的时，每一个时间步，也就是在单个Episode内每一次个体在状态S_t采取一个行为后都要更新Q值，同样使用 $\\epsilon$ -贪婪探索的形式来改善策略。 $Q(S,A) \\gets Q(S,A) + \\alpha(R+ \\gamma Q(S^{‘},A^{‘})-Q(S,A))$ On-Policy的 SARSA 算法 定理：满足如下两个条件时，Sarsa算法将收敛至最优行为价值函数。 条件一：任何时候的策略 $\\pi_t(a|s)$ 符合 GLIE 特性; 条件二：步长系数αt满足：$\\sum_{t=1}^{\\infty}{a_t} = \\infty $ 且 $\\sum_{t=1}^{\\infty}{a_t^2} &lt; \\infty $ Sarsa(λ)算法：https://zhuanlan.zhihu.com/p/28108498 Off-Policy Learning现时策略学习的特点就是当前遵循的策略就是个体学习改善的策略。Off-Policy学习（Off-Policy Learning）则指的是在遵循一个策略\\mu(a|s)的同时评估另一个策略\\pi(a|s)，也就是计算确定这另一个策略下的状态价值函数v_{\\pi}(s)或状态行为价值函数q_{\\pi}(s, a)。为什么要这么做呢？因为这样可以较容易的从人类经验或其他个体的经验中学习，也可以从一些旧的策略中学习，可以比较两个策略的优劣。其中可能也是最主要的原因就是遵循一个探索式策略的基础上优化现有的策略。同样根据是否经历完整的Episode可以将其分为基于蒙特卡洛的和基于TD的。基于蒙特卡洛的Off-Policy学习仅有理论上的研究价值，在实际中毫无用处。在解释这一结论时引入了“重要性采样（importance sampling）”这个概念，这里就不详述了，有兴趣的读者可以参考原讲义。这里主要讲解常用的TD下的Off-Policy学习。 Off-Policy TD学习Off-PolicyTD学习的任务就是使用TD方法在遵循一个策略 $\\mu(a|s)$ 的同时评估另一个策略 $\\pi(a|s)$。具体数学表示为： 这个公式可以这样解释：个体处在状态S_t中，基于策略 $\\mu$ 产生了一个行为 $A_t$ ，执行该行为后进入新的状态 $S_{t+1}$，那么在当前策略下如何根据新状态的价值调整原来状态的价值呢？Off-Policy的方法就是，在状态S_t时比较分别依据另一个策略 $\\pi$ 和当前遵循的策略 $\\mu$ 产生行为 $A_t$ 的概率大小，如果策略 $\\pi$ 得到的概率值与遵循当前策略 $\\mu$ 得到的概率值接近，说明根据状态 $S_{t+1}$ 价值来更新 $S_t$ 的价值同时得到两个策略的支持，这一更新操作比较有说服力。同时也说明在状态S_t时，两个策略有接近的概率选择行为 $A_t$。假如这一概率比值很小，则表明如果依照被评估的策略，选择 $A_t$ 的机会很小，这时候我们在更新 $S_t$ 价值的时候就不能过多的考虑基于当前策略得到的状态 $S_{t+1}$ 的价值。同样概率比值大于1时的道理也类似。这就相当于借鉴被评估策略的经验来更新我们自己的策略。 应用这种思想最好的方法是基于 TD(0) 的Q-学习（Q-learning）。它的要点在于，更新一个状态行为对的Q价值时，采用的不是当前遵循策略的下一个状态行为对的Q价值，而是采用的待评估策略产生的下一个状态行为对的Q价值。公式如下： $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha(R_{t+1}+ \\gamma Q(S_{t+1},A^{‘})-Q(S_t,A_t))$ 式中，TD目标是基于另一个估策略 $\\pi$ 产生的行为A’得到的Q价值。Q学习最主要的表现形式是：个体遵循的策略是基于当前状态行为价值函数Q(s,a)的一个 $\\epsilon-greedy$ 策略，而目标策略是基于当前状态行为价值函数Q(s,a)不包含 $\\epsilon$ 的单纯greedy策略： $ \\pi (S_{t+1})= \\arg\\max_{a^{‘}} Q(S_{t+1},s^{‘})$ 这样Q学习的TD目标值可以被大幅简化： 这样在状态 $S_t$ 依据Ɛ-greedy遵循策略得到的行为 $A_t$ 的 Q 价值将朝着 $S_{t+1}$ 状态所具有的最大Q价值的方向做一定比例的更新。这种算法能够使 greedy 策略 $\\pi$ 最终收敛到最佳策略。由于个体实际与环境交互的时候遵循的是 $\\epsilon-greedy$ 策略，它能保证经历足够丰富的新状态。 定理：Q学习控制将收敛至最优状态行为价值函数：$Q(s,a) \\rightarrow q_*(s,a)$。 下图是Q学习具体的更新公式和图解： 下图是Q学习的算法流程： 总结DP与TD关系下面两张图概括了各种DP算法和各种TD算法，同时也揭示了各种不同算法之间的区别和联系。总的来说TD是采样+有数据引导(bootstrap)，DP是全宽度+实际数据。如果从Bellman期望方程角度看：聚焦于状态本身价值的是迭代法策略评估（DP）和TD学习，聚焦于状态行为对价值函数的则是Q-策略迭代（DP）和SARSA；如果从针对状态行为价值函数的Bellman优化方程角度看，则是Q-价值迭代（DP）和Q学习。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 4 模型无关预测 Model-Free Prediction","date":"2017-10-15T13:01:30.000Z","path":"2017/10/15/David_Silver_RL_4/","text":"简介 Introduction上节课中通过动态规划能够解决已知 environment 的 MDP 问题，也就是已知 $S,A,P,R,\\gamma$，其中根据是否已知 policy 将问题又划分成了 prediction 和 control 问题，本质上来说这种 known MDP 问题已知 environment 即转移矩阵与 reward 函数，但是很多问题中 environment 是未知的，不清楚做出了某个 action 之后会变到哪一个 state 也不知道这个 action 好还是不好，也就是说不清楚 environment 体现的 model 是什么，在这种情况下需要解决的 prediction 和 control 问题就是Model-free prediction和Model-free control。显然这种新的问题只能从与 environment 的交互得到的 experience 中获取信息。 这节课要解决的问题是Model-free prediction，即未知environment的Policy evaluation，在给定的 policy 下，每个state的 value function 是多少。 蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning蒙特卡洛强化学习是假设每个 state 的 value function 取值等于多个 episodes 的 return Gt 的平均值，它需要每个 episode 是完整的流程，即一定要执行到终止状态。 蒙特卡洛策略评估 Monte-Carlo Policy Evaluation目标： 在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。 数学描述如下： 基于特定策略 $\\pi$ 的一个 Episode信 息可以表示为如下的一个序列：$S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, …, S_{t}, A_{t}, R_{t+1}, …, S_{k}$ ~ $\\pi$ t 时刻状态 $S_{t}$ 的收获：$G_{t} = R_{t+1} + \\gamma R_{t+2} + … + \\gamma^{T-1} R_{T}$其中 T 为终止时刻。 该策略下某一状态 s 的价值：$v_{\\pi}(s) = E_{\\pi} [ G_{t} | S_{t} = s ]$ 在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法： 首次访问蒙特卡洛策略评估在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，仅当该状态第一次出现时列入计算: 状态出现的次数加1： $N(s) \\leftarrow N(s) + 1$总的收获值更新： $S(s) \\leftarrow S(s) + G_{t}$状态s的价值： $V(s) = S(s) / N(s)$当 $N(s) \\rightarrow \\infty$ 时， $V(s) \\rightarrow v_{\\pi}(s)$ 每次访问蒙特卡洛策略评估在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，状态s每次出现在状态转移链时，计算的具体公式与上面的一样，但具体意义不一样。 状态出现的次数加1： $N(s) \\leftarrow N(s) + 1$总的收获值更新： $S(s) \\leftarrow S(s) + G_{t}$状态s的价值： $V(s) = S(s) / N(s)$当 $N(s) \\rightarrow \\infty$ 时， $V(s) \\rightarrow v_{\\pi}(s)$ 累进更新平均值 Incremental Mean这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。 理论公式如下： 这个公式比较简单。把这个方法应用于蒙特卡洛策略评估，就得到下面的蒙特卡洛累进更新。 蒙特卡洛累进更新对于一系列 Episodes 中的每一个： $S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, …, S_{t}, A_{t}, R_{t+1}, …, S_{k} $ 对于Episode里的每一个状态 $S_{t}$ ，有一个收获 $G_{t}$ ，每碰到一次 $S_{t}$ ,使用下式计算状态的平均价值 $V(S_{t})$ ：其中： 在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的 Episode 信息。此时可以引入参数 $\\alpha$ 来更新状态价值： 以上就是蒙特卡洛学习方法的主要思想和描述，由于蒙特卡洛学习方法有许多缺点（后文会细说），因此实际应用并不多。接下来介绍实际常用的TD学习方法。 时序差分学习 Temporal-Difference Learning时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习不完整的 Episode ，通过自身的引导（bootstrapping），猜测 Episode 的结果，同时持续更新这个猜测。 我们已经学过，在Monte-Carlo学习中，使用实际的收获（return） $G_{t} $来更新价值（Value）：$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha (G_{t} - V(S_{t}))$ 在 TD 学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$ 与下一状态 $S_{t+1}$ 的预估状态价值乘以衰减系数 $\\gamma$ 组成，这符合 Bellman 方程的描述：$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t}))$ 式中：$R_{t+1} + \\gamma V(S_{t+1})$ 称为 TD 目标值$\\delta_{t} = R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})$ 称为 TD 误差 BootStrapping 指的就是TD目标值 $R_{t+1} + \\gamma V(S_{t+1})$ 代替收获 $G_t$ 的过程，暂时把它翻译成“引导”。 示例——驾车返家 想象一下你下班后开车回家，需要预估整个行程花费的时间。假如一个人在驾车回家的路上突然碰到险情：对面迎来一辆车感觉要和你相撞，严重的话他可能面临死亡威胁，但是最后双方都采取了措施没有实际发生碰撞。如果使用蒙特卡洛学习，路上发生的这一险情可能引发的负向奖励不会被考虑进去，不会影响总的预测耗时；但是在TD学习时，碰到这样的险情，这个人会立即更新这个状态的价值，随后会发现这比之前的状态要糟糕，会立即考虑决策降低速度赢得时间，也就是说你不必像蒙特卡洛学习那样直到他死亡后才更新状态价值，那种情况下也无法更新状态价值。 TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。 基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来更新价值函数（各个状态的价值）。这里使用的是从某个状态预估的到家还需耗时来间接反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；而对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。 Monte-Carlo VS. Temporal Difference在谈两种算法的优劣前，先谈谈 Bias/Variance tradeoff 的问题。平衡 Bias/Variance 是机器学习比较经典的一个问题，bias 是指预测结果与真实结果的差值，variance 是指训练集每次预测结果之间的差值，bias 过大会导致欠拟合它衡量了模型是否准确，variance 过大会导致过拟合衡量了模型是否稳定。如果 $G_t$ 和 $R_{t+1} + \\gamma v_{\\pi} (S_{t+1})$ 跟真实值一样，那么就是无偏差估计。因为在MC算法中，它是将最终获得的 reward 返回到了前面的状态，因此是真实值，但是它采样的 episode 并不能代表所有的情况，所以会导致比较大的 variance 。而 TD的 $R_{t+1} + \\gamma v_{\\pi} (S_{t+1})$ 跟真实值是有偏差的，在计算的过程基于随机的状态、转移概率、reward 等等，涵盖了一些随机的采样，因此 variance 比较小。 示例——AB已知：现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。 问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即V(A)=？， V(B)=？ 答案：V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。 解释：应用MC算法，由于需要完整的 Episode ,因此仅 Episode1 可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。应用 TD 算法时，TD 算法试图利用现有的 Episode 经验构建一个 MDP（如下图），由于存在一个 Episode 使得状态A有后继状态 B ，因此状态A的价值是通过状态B的价值来计算的，同时经验表明 A 到 B 的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于 B 的状态价值。 确定性等价 Certainty EquivalenceMC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案，这一均方差用公式表示为： 式中，k 表示的是 Episode 序号， K 为总的 Episode 数量， t 为一个 Episode 内状态序号（第1,2,3…个状态等）， $T_{k}$ 表示的是第 k 个Episode总的状态数， $G^{k}_{t}$ 表示第 k 个 Episode 里 t 时刻状态 $S_{t}$ 获得的最终收获， $V(S^{k}_{t})$ 表示的是第 k 个 Episode 里算法估计的 t 时刻状态 $S_{t}$ 的价值。 TD算法则收敛至一个根据已有经验构建的最大可能的马儿可夫模型的状态价值，也就是说TD算法将首先根据已有经验估计状态间的转移概率： 同时估计某一个状态的即时奖励： 最后计算该MDP的状态函数。 三种强化学习算法Monte-Carlo, Temporal-Difference 和 Dynamic Programming 都是计算状态价值的一种方法，区别在于，前两种是在不知道Model的情况下的常用方法，这其中又以MC方法需要一个完整的Episode来更新状态价值，TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态S’及其转移概率以及对应的即时奖励来计算这个状态S的价值。 关于是否采用 Bootstrap： MC 没有引导数据，只使用实际收获；DP和TD都有引导数据。 关于是否用采样 Sampling: MC和TD都是应用样本来估计实际的价值函数；而DP则是利用模型直接计算得到实际价值函数，没有样本或采样之说。 下面的几张图直观地体现了三种算法的区别： MC: 采样，一次完整经历，用实际收获更新状态预估价值 TD：采样，经历可不完整，用喜爱状态的预估状态价值预估收获再更新预估价值 DP：没有采样，根据完整模型，依靠预估数据更新状态价值 上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。 当使用单个采样，同时不走完整个Episode就是TD； 当使用单个采样但走完整个Episode就是MC； 当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP； 当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。 需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。 TD(λ)TD(0) 是指在某个状态 s 下执行某个动作后转移到下一个状态 s′ 时，估计 s′ 的 return 再更新 s ，假如 s 之后执行两次动作转移到 s″ 时再反回来更新s的值函数，那么就是另一种形式，从而根据 step 的长度 n 可以扩展 TD 到不同的形式，当 step 长度到达当前 episode 终点时就变成了 MC。 定义 n-step 收获：$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + … + \\gamma ^{n-1}R_{t+n}+\\gamma ^nV(S_{t+n})$ n = 1 时 即 TD(0) 那么，n-step TD 学习状态价值函数的更新公式为：$V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^{(n)} - V(S_t))$ 既然存在 n-step 预测，那么n=？时效果最好呢? 前向认知 TD(λ) The Forward View of TD(λ)如果将不同的 n 对应的 $G_t^{(n)}$ 平均一下，这样能够获得更加 robust 的结果，而为了有效的将不同 $G_t^{(n)}$ 结合起来，对每个 n 的 $G_t^{(n)}$ 都赋了一个权重 $1-\\lambda,(1-\\lambda)\\lambda,…,(1-\\lambda)\\lambda^{n-1},\\lambda^{T-t-1}$ ，T 是状态的总数, t 表示了在第几个状态 ，所有的权重加起来为1,这样又能得到一组更新 value function 的公式。 $G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty }\\lambda^{n-1}G_t^{(n)}$为了更好的把episode的terminal state体现出来，我们可以写成下式：$G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{T-t-1 }\\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1}G_t^{(T-t)}$ $V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^{\\lambda} - V(S_t))$ 下图是各步收获的权重分配图 对于每个访问到的state，我们都是从它开始向前看所有的未来reward，并决定如何结合这些reward来更新当前的state。每次我们更新完当前state，我们就到下一个state，永不再回头关心前面的state。这种感觉就像下图一样： TD(λ)对于权重分配的图解 对于 n=3 的 3-步收获，赋予其在 $\\lambda$ 收获中的权重如左侧阴影部分面积，对于终止状态的 T-步 收获，T以后的所有阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。 在实际任务中这类算法却很少用，因为不便于实现：在n-step TD中，你需要等待 n 步之后观测得到的reward和state。如果n很大，这个等待过程就是个问题，存在很大的滞后性，这是一个待解决问题。那么n-step TD有什么意义呢？n-step TD的思想可以让我们更好理解接下来的”资格迹“方法。 后向认知 TD(λ) The Backward View of TD(λ)TD(λ)的后向视角非常有意义，因为它在概念上和计算上都是可行而且简单的。具体来说，前向视角只提供了一个非常好但却无法直接实现的思路，因为它在每一个timestep都需要用到很多步之后的信息，这在工程上很不高效。而后向视角恰恰解决了这个问题，采用一种带有明确因果性的递增机制来实现TD(λ)，最终的效果是在on-line case和前向视角近似，在off-line case和前向视角精确一致。 后向视角的实现过程中，引入了一个额外的和每个state都相关的存储变量，它的名字就叫做“资格迹”。在t时刻的状态s对应的资格迹，标记为 $Z_t(s) \\in \\mathbb{R}^+$ 。资格迹的更新方式如下： 稍微解释一下上面式子的含义：在一开始，每个episode中的所有状态都有一个初始资格迹，然后时间开始走。到下一个timestep，被访问到的那个状态，其资格迹为前一个时刻该状态资格迹乘上一个（decay param 乘 discount param），然后加1，表示当前时刻，该状态的资格迹变大；其他未被访问的状态，其资格迹都只是在原有基础上乘以（decay param*discount param），不用加1,表明它们的资格迹都退化了。 我们把λ成为“迹退化参数”，把上式这种更新方式的资格迹叫做“累加型资格迹”。因为它在状态被访问的时候累加，不被访问的时候退化。如下图： 常规的TD误差如下式：$\\delta_t = R_{t+1} + \\gamma V_t(S_{t+1})-V_t(S_t)$ 但是在TD(λ)的后向视角中，这个误差却可以影响所有“最近”访问过的状态的，对于这些状态来说，TD误差如下： $\\Delta V_t(s) = \\alpha \\delta_tZ_t(s) \\;\\;for \\; all \\; s\\in S$ 可以这么说：后向视角的TD(λ)，才是真正的TD(λ)。我们来更形象地表述一下后向视角的过程：每次在当前访问的状态得到一个误差量的时候，这个误差量都会根据之前每个状态的资格迹来分配当前误差。这就像是一个小人，拿着当前的误差，然后对准前面的状态们按比例扔回去，就像下图一样： λ=0:TD(λ)退化成TD(0)，除了当前状态，其他所有状态的资格迹为0。这时候，小人不再向前面扔误差量，而是只给当前状态了。随着λ变大，但是小于1,更多状态的误差被当前误差所影响，越远的，影响越小。我们称越远的状态被分配的credit（可以理解为信用）越少。 λ=1:当 $\\gamma$ 不等于1时， $\\delta_t$ 依照 $\\gamma^k$ 逐步递减；如果 $\\gamma$ 等于1，那么资格迹就不再递减。我们把λ=1时的TD算法也叫做TD(1)。 传统MC如果不走到episode结束，是什么都学不到的。而且传统MC只适用于非连续式，有明确终点状态的任务。再看TD(1)，读者可以把它想象成拖着一条移动的长尾巴的MC类算法（你可以把传统MC看成是有一条固定的长尾巴算法），可以即时地从正在进行的episode中学习，基于它的控制方法可以及时地修正后续episode的生成方式。因此TD(1)不仅适用于片段式任务，也适用于连续性任务。 Equivalence of Forward and Backward Views参考：https://zhuanlan.zhihu.com/p/27773020 小结下表给出了λ取各种值时，不同算法在不同情况下的关系。 内容的图标总结：","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 3 动态规划解决MDP的Planning问题","date":"2017-09-21T13:01:30.000Z","path":"2017/09/21/David_Silver_RL_3/","text":"简介 Introduction当问题具有下列特性时，通常可以考虑使用动态规划来求解： 第一个特性是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解； 子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。 马尔可夫决策过程（MDP）具有上述两个属性：Bellman方程把问题递归为求解子问题，价值函数就相当于存储了一些子问题的解，可以复用。因此可以使用动态规划来求解MDP。 我们用动态规划算法来求解一类称为“规划 Planning”的问题。“规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用规划来解决 Predict 和 Control 问题。 策略迭代 Policy Iteration这个解决途径主要分为两步： Policy Evaluation:基于当前的Policy计算出每个状态的value function $V$ （迭代计算直到收敛）\b Policy Improvment:基于当前的value function，采用贪心算法来找到当前最优的Policy $\\pi$ 如此反复多次，最终得到最优策略 $\\pi^{star}$ 和最优状态价值函数 $V^{star} $ ( star 代表 \b* )。 下图是一个叫Small Gridworld的例子，左上角和右下角是终点，γ=1，移动一步 reward=-1，起始的random policy是朝每个能走的方向概率相同。 策略改善 Policy Improvment 理论证明思考：很多时候，策略的更新较早就收敛至最优策略，而状态价值的收敛要慢很多，是否有必要一定要迭代计算直到状态价值得到收敛呢？ 值迭代 Value Iteration优化原则 Principle of Optimality一个最优策略可以被分解为两部分：从状态 s 到下一个状态 s’ 采取了最优行为 $A_*$ ；在状态 s’ 时遵循一个最优策略。 从上面原理出发，如果已知子问题的最优值 v∗(s′)，那么就能通过第一个Bellman Optimality Equation将 v∗(s) 也推出来。因此从终点开始向起点推就能把全部状态最优值推出来。Value Iteration 通过迭代的方法，通过这一步的 $v_k (s’)$ 更新下一步的 $v_{k+1}(s)$ ，最终收敛到最优的 v∗ ，需要注意的是中间生成的value function的值不对应着任何 policy。 考虑下面这个Shortest Path例子，左上角是终点，要求的是剩下每一个格子距离终点的最短距离，每走一步，reward=-1 因此，针对MDP要解决的两个问题，有如下几种方式来解决。针对prediction，因为它的目标是在已知的Policy下得到收敛的value function，因此针对问题不断迭代计算Bellman Expectation Equation就够了，但是control则需要同时获得最优的policy，那么在Iterative Policy Evaluation的基础上加入一个选择Policy的过程就行了，也就是上面的Policy Iteration，另外Value Iteration虽然在迭代的过程中没有显式计算出policy，但是在得到最优的value function之后就能推导出最优的policy，因此也能用做解决control问题。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"学徒学习 Apprenticeship learning via inverse reinforcement learning","date":"2017-09-15T13:01:30.000Z","path":"2017/09/15/Apprenticeship_learing/","text":"学徒学习是Ng（吴恩达）和Abbeel提出来的。学徒学习是这样：Agent从专家示例中学到回报函数，使得在该回报函数下所得到的最优策略在专家示例策略附近。 回报函数$R(s)$ 假设为：$R\\left(s\\right)=w^T\\cdot\\phi\\left(s\\right)$，其中$\\phi(s)$为映射特征的基函数，可以为多项式基底，也可以为傅里叶基底。文中是以线性函数为基底。逆向强化学习求的就是回报函数中的系数w。策略 $\\pi$ 的值函数为：$v_{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t R(s_t)]$将回报函数代入：$v_{\\pi}(s) = w^T E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$ 将上式右半部分定义为特征期望：$\\mu\\left(\\pi\\right)=E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^t \\phi(s_t)]$。需要注意的是，特征期望跟策略 $\\pi$ 有关，策略不同时，策略期望也不相同 当给定m条专家轨迹后，根据定义我们可以估计专家策略的特征期望为：$\\hat{\\mu}_E=\\frac{1}{m}\\Sigma_{i=1}^{m}\\Sigma_{t=0}^{\\infty}\\gamma^t\\phi\\left(s_{t}^{\\left(i\\right)}\\right)$其中，专家状态序列为专家轨迹:${ s_{0}^{(i)},s_{1}^{(i)},\\cdots }_{i=1}^{m}$ 逆向强化学习可以归结为如下问题： 找到一个策略，使得该策略的表现与专家策略相近。我们可以利用特征期望来表示一个策略的好坏，找到一个策略，使其表现与专家策略相近，其实就是找到一个策略 $\\tilde{\\pi}$ 的特征期望与专家策略的特征期望相近，即使如下不等式成立：$\\lVert\\mu\\left(\\tilde{\\pi}\\right)-\\mu_E\\rVert_2\\le\\epsilon$ 当该不等式成立时，对于任意的权重$\\lVert w\\rVert_1\\le 1$，值函数满足如下不等式： 算法 其中第二行的目标函数为: $t^{(i)}=\\max_{w:\\lVert w\\rVert_2\\le 1}\\min_{j\\in{0\\cdots(i-1)}}w^T(\\mu_E-\\mu^{(j)})$ 写成标准的优化形式为：注意，在进行第二行求解时，$\\mu^{\\left(j\\right)}$中的 $j\\in{0,1,\\cdots ,i-1}$是前i-1次迭代得到的最优策略。也就是说第i次求解参数时，i-1次迭代的策略是已知的。这时候的最优函数值t相当于专家策略 $\\mu_E$ 与i-1个迭代策略之间的最大边际。 我们可以从支持向量机的角度去理解。专家策略为一类，其他策略为另一类，参数的求解其实就是找一条超曲面将专家策略和其他策略区分开来。这个超平面使得两类之间的边际最大。 第四行是在第二行求出参数后，便有了回报函数$R=\\left(w^{\\left(i\\right)}\\right)^T\\phi$，利用该回报函数进行强化学习，从而得到该回报函数下的最优策略 $\\pi^{\\left(i\\right)}$。 总结学徒逆向强化学习方法分为两步:第一步在已经迭代得到的最优策略中，利用最大边际方法求出当前的回报函数的参数值；（该计算需要用到QP(二次规划)求解器或者\bSVM求解器。文中也给出了一种不\b使用\b求解器的简单算法。）第二步利用求出的回报函数的参数值进行正向强化学习方法求得当前最优的策略，然后重复第一步。 需要注意的是，$\\phi(s)$ 中输入的 s \b为 i 个特征：$s^1,s^2,…,s^i$，if 第 i 个特征存在，$s^i = 1$，else $s^i = 0$。又因为$\\lVert w\\rVert_1\\le 1$，所以$R\\le 1$。 ref: Abbeel, P., &amp; Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. International Conference on Machine Learning(Vol.11, pp.1). ACM.","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 2 MDP","date":"2017-08-18T13:01:30.000Z","path":"2017/08/18/David_Silver_RL_2/","text":"在强化学习中，马尔可夫决策过程（Markov decision process, MDP）是对完全可观测的环境进行描述的，也就是说观测到的状态内容完整地决定了决策的需要的特征。几乎所有的强化学习问题都可以转化为MDP。本讲是理解强化学习问题的理论基础。 马尔可夫过程 Markov Process马尔可夫性 Markov Property 某一状态信息包含了所有相关的历史 只要当前状态可知，历史信息 history 就可以被丢弃 当前状态就可以决定未来 The future is independent of the past given the present即该状态具有马尔可夫性。 可以用下面的状态转移概率公式来描述马尔可夫性：$P_{ss’} = P[S_{t+1}=s’|S_t=s]$ 下面的状态转移矩阵定义了所有状态的转移概率：式中n为状态数量，矩阵中每一行元素之和为1. 马尔可夫过程 Markov Process马尔可夫过程 又叫马尔可夫链(Markov Chain)，它是一个无记忆的随机过程，可以用一个元组表示，其中S是有限数量的状态集，P是状态转移概率矩阵。 如下图1圆圈内是状态，箭头上的值是状态之间的转移概率。class是指上第几堂课，facebook指看facebook网页，pub指去酒吧，pass指通过考试，sleep指睡觉。例如处于class1有0.5的概率转移到class2，或者0.5的概率转移到facebook。 从而可以产生非常多的随机序列，例如C1 C2 C3 Pass Sleep或者C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep等。这些随机状态的序列就是马尔可夫过程。 马尔可夫奖励过程 Markov Reward Process马尔可夫奖励过程在马尔可夫过程的基础上增加了奖励 R 和衰减系数 γR是一个奖励函数。S 状态下的奖励是某一时刻(t)处在状态s下在下一个时刻 (t+1) 能获得的奖励期望：$R_{s} = E[R_{t+1} | S_{t} = s ]$ 衰减系数 Discount Factor: γ∈ [0, 1]，其远期利益具有一定的不确定性，符合人类对于眼前利益的追求等。 回报 Return定义：回报 $G_{t}$ 为在一个马尔可夫奖励链上从 t 时刻开始往后所有的奖励的有衰减的总和。公式如下：$G_t = R_{t+1}+\\gamma R_{t+2}+…=\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$其中衰减系数体现了未来的奖励在当前时刻的价值比例，在k+1时刻获得的奖励R在t时刻的体现出的价值是 $\\gamma^k R$ ，γ 接近0，则表明趋向于“近视”性评估；γ 接近1则表明偏重考虑远期的利益。 价值函数 Value Function状态值函数给出了某一状态或某一动作的长期价值。定义：一个马尔可夫奖励过程中的状态值函数为从该状态开始的马尔可夫链回报的期望：$v(s) = E [ G_{t} | S_{t} = s ]$ 价值函数的推导Bellman方程 - MRP先尝试用价值的定义公式来推导看看能得到什么：这个推导过程相对简单，仅在导出最后一行时，将 $G_{t+1}$ 变成了 $v(S_{t+1})$ 通过方程可以看出 v(s) 由两部分组成，一是该状态的即时奖励期望，即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关；另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。如果用s’表示s状态下一时刻任一可能的状态，那么Bellman方程可以写成：$v(s) = E[G_t|S_t=s]= E[R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3}+…) | S_t=s]= E[R_{t+1} + \\gamma G_{t+1} | S_t=s] = ER_s + \\gamma \\sum_{s’\\in S}P_{ss’}v(s’) $ 下图已经给出了 γ=1 时各状态的价值（该图没有文字说明 γ=1，根据视频讲解和前面图示以及状态方程的要求，γ 必须要确定才能计算），状态 $C_{3}$ 的价值可以通过状态 Pub 和 Pass 的价值以及他们之间的状态转移概率来计算：$4.3 = -2 + 1.0 ( 0.6 10 + 0.4 * 0.8 )$ Bellman方程的矩阵形式和求解实际上，计算复杂度是 $O(n^{3})$ ， n 是状态数量。因此直接求解仅适用于小规模的MRPs。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference，后文会逐步讲解这些方法。 马尔可夫决策过程 Markov Decision Process相较于马尔可夫奖励过程，马尔可夫决策过程多了一个动作（动作）集合A，它是这样的一个元组: 。看起来很类似马尔可夫奖励过程，但这里的P和R都与具体的动作a对应，而不像马尔可夫奖励过程那样仅对应于某个状态，A表示的是有限的动作的集合。具体的数学表达式如下：$P^a_{ss’} = P[S_{t+1}=s’|S_t=s,A_t=a]$$R^a_{s} = E[R_{t+1} | S_{t} = s,A_t=a ]$ 下图给出了一个可能的MDP的状态转化图。图中红色的文字表示的是采取的动作，而不是先前的状态名。对比之前的学生MRP示例可以发现，即时奖励与动作对应了，同一个状态下采取不同的动作得到的即时奖励是不一样的。由于引入了Action，容易与状态名混淆，因此此图没有给出各状态的名称；此图还把Pass和Sleep状态合并成一个终止状态；另外当选择”去查阅文献”这个动作时，主动进入了一个临时状态（图中用黑色小实点表示），随后被动的被环境按照其动力学分配到另外三个状态，也就是说此时Agent没有选择权决定去哪一个状态。 策略 Policy $\\pi$策略 $\\pi$是概率的集合或分布，其元素 $\\pi(a|s)$ 为对过程中的某一状态s采取可能的动作 a 的概率。用 $\\pi(a|s)$ 表示。 一个策略完整定义了 Agent 的动作方式，也就是说定义了 Agent 在各个状态下的各种可能的动作方式以及其概率的大小。Policy 仅和当前的状态有关，与历史信息无关；同时某一确定的Policy是静态的，与时间无关；但是 Agent 可以随着时间更新策略。 当给定一个MDP: M = &lt; S, A, P, R, $\\gamma$ &gt; 和一个策略 $\\pi$，那么状态序列 $S_{1},S_{2},…$ 是一个马尔可夫过程 &lt; $S$,$P^{\\pi}$&gt; :状态转移概率公式表示: $P^\\pi_{ss’}=\\sum_{a\\in A}\\pi(a|s)P^a_{ss’}$在执行策略 $\\pi$ 时，状态从 s 转移至 s’ 的概率等于一系列概率的和，这一系列概率指的是在执行当前策略时，执行某一个动作的概率与该动作能使状态从s转移至s’的概率的乘积。 同样的，状态和奖励序列 $S_{1}, R_{2}, S_{2}, R_{3}, S_{3}, … $是一个马尔可夫奖励过程 &lt; $S$, $P^{\\pi}$, $R^{\\pi}$, $\\gamma$ &gt;奖励函数表示：$R^\\pi_{s}=\\sum_{a\\in A}\\pi(a|s)R^a_{s}$当前状态s下执行某一指定策略得到的即时奖励是该策略下所有可能动作得到的奖励与该动作发生的概率的乘积的和。 策略 $\\pi$ 在MDP中的作用相当于 agent 可以在某一个状态时做出选择，进而有形成各种马尔可夫过程的可能，而且基于策略产生的每一个马尔可夫过程是一个马尔可夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。 基于策略 $\\pi$ 的价值函数 Value Function状态值函数 State-Value Function V定义 $v_\\pi(s)$ 是在 MDP 下的基于策略 $\\pi$ 的状态值函数，表示从状态s开始，遵循当前策略时所获得的收获的期望；或者说在执行当前策略 $\\pi$ 时，衡量个体处在状态s时的价值大小。数学表示如下：$v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]$ 注意策略是静态的、关于整体的概念，不随状态改变而改变；变化的是在某一个状态时，依据策略可能产生的具体动作，因为具体的动作是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同动作的概率。 动作值函数 Action-Value Function Q定义 $q_{\\pi}(s,a)$ 为动作值函数，表示在执行策略 $\\pi$ 时，对当前状态 s 执行某一具体动作 a 所能的到的收获的期望；或者说在遵循当前策略 $\\pi$ 时，衡量对当前状态执行动作 a 的价值大小。动作值函数一般都是与某一特定的状态相对应的。动作值函数的公式描述如下:$q_{\\pi}(s,a)= E_{\\pi}[G_t|S_t=s, A_t=a]$ 由于策略$\\pi(a|s)$是可以改变的，因此两个值函数的取值不像MRP一样是固定的，那么就能从不同的取值中找到一个最大值即最优值函数(这节课没有讲如何求解)。MDP需要解决的问题并不是每一步到底会获得多少累积reward，而是找到一个最优的解决方案。 Bellman期望方程 Bellman Expectation Equation根据这两个值函数的定义，它们之间的关系表示为 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s,a)$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’)$ 第二个式子是说当选择一个action之后，转移到不同状态下之后获取的reward之和是多少。将两个式子互相代入，可以得到如下的Bellman期望方程。 $v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)(R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’))$ $q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^a\\sum_{a’\\in A}\\pi(a’|s’)q_{\\pi}(s’,a’)$ 下图解释了红色空心圆圈状态的状态价值是如何计算的，遵循的策略随机策略，即所有可能的动作有相同的几率被选择执行。和MRP类似的,我们也可以得到矩阵形式和求解 最优价值函数最优状态值函数 $v$ 指的是在从所有策略产生的状态值函数中，选取使状态 s 值最大的函数：$v_* = \\max \\limits_{\\pi} v_{\\pi}(s)$ 类似的，最优动作值函数 $q (s,a)$ 指的是从所有策略产生的动作值函数中，选取是状态动作对 &lt;$s$, $a$&gt; 价值最大的函数：$q_* (s,a) = \\max \\limits_{\\pi} q_{\\pi}(s,a)$ 最优价值函数明确了MDP的最优可能表现，当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。 最优策略当对于任何状态 s，遵循策略 $\\pi$ 的价值不小于遵循策略 $\\pi$’ 下的价值，则策略 $\\pi$ 优于策略 $\\pi$’：$\\pi \\geq \\pi^{‘} if v_{\\pi}(s)\\geq v_{\\pi^{‘}}(s) ,\\forall s $定理 对于任何MDP，下面几点成立： 存在一个最优策略，比任何其他策略更好或至少相等； 所有的最优策略有相同的最优价值函数； 所有的最优策略具有相同的动作值函数。 寻找最优策略可以通过最大化最优动作值函数来找到最优策略：对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优动作值函数，则表明我们找到了最优策略。 Bellman最优方程 Bellman Optimality Equation针对 $v$ 一个状态的最优价值等于从该状态出发采取的所有动作产生的动作价值中最大的那个动作价值：针对 $q$，在某个状态 s 下，采取某个动作的最优价值由2部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和： 组合起来，得到Bellman最优方程: 满足bellman最优方程，意味着找到了最优策略。也就是$v_\\pi (s) = \\max \\limits_{a} q_{\\pi}(s,a)$，也就是不需要在进行策略改进。 求解Bellman最优方程Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。后续会逐步讲解展开。 MDP延伸——Extensions to MDPs","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"David Silver 强化学习 1 Introduction","date":"2017-08-14T13:01:30.000Z","path":"2017/08/14/David_Silver_RL_1/","text":"强化学习的特点(不同于其他机器学习)： 没有监督数据、只有奖励信号 奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。 时间（序列）是一个重要因素 当前的行为影响后续接收到的数据 The RL Problem奖励 Reward 一个Reward $R_{t}$ 是信号的反馈，是一个标量 它反映 Agent 在 t 时刻做得怎么样 Agent 的工作就是最大化累计奖励 强化学习主要基于这样的”奖励假设”：所有问题解决的目标都可以被描述成最大化累积奖励。 序列决策 Sequential Decision Making 目标：选择一系列的 Action 以最大化未来的总体奖励 这些 Action 可能是一个长期的序列 奖励可能而且通常是延迟的 有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励 个体和环境 Agent &amp; Environment可以从个体和环境两方面来描述强化学习问题。在 t 时刻，Agent 可以： 做出一个行为 $A_{t}$ 有一个对于环境的观察评估 $O_{t}$ 从环境得到一个奖励信号 $R_{t}$ Environment 可以： 接收 Agent 的动作 $A_{t}$ 更新环境信息，同时使得Agent可以得到下一个观测 $O_{t+1}$ 给Agent一个奖励信号 $R_{t+1}$ 历史和状态 History &amp; State历史 历史是观测、行为、奖励的序列： $H_{t} = O_{1}, R_{1}, A_{1},…, O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}$ 状态 状态是所有能够决定将来的已有的信息，是关于历史的一个函数：$S_{t} = f(H_{t})$ 环境状态 Environment State 是环境的私有 representation 包括环境用来决定下一个观测/奖励的所有数据 通常对Agent并不完全可见，也就是Agent有时候并不知道环境状态的所有细节 即使有时候环境状态对Agent可以是完全可见的，这些信息也可能包含着一些无关信息 个体状态 Agent State 是Agent的内部representation 包括Agent可以使用的、决定未来动作的所有信息 Agent State是强化学习算法可以利用的信息 它可以是历史的一个函数： $S^{a}_{t} = f(H_{t})$ 信息状态 Information State包括历史上所有有用的信息，又称Markov状态。也就是说: 如果信息状态是可知的，那么历史可以丢弃，仅需要 t 时刻的信息状态就可以了。例如：环境状态是Markov的，因为环境状态是环境包含了环境决定下一个观测/奖励的所有信息 同样，（完整的）历史 $H_{t}$ 也是Markov的。 完全可观测的环境 Fully Observable Environments Agent能够直接观测到环境状态: $O_{t} = S^{a}_{t} = S^{e}_{t}$ 正式地说，这种问题是一个马尔可夫决策过程（Markov Decision Process， MDP） 部分可观测的环境 Partially Observable Environments Agent 间接观测环境。举了几个例子： 一个可拍照的机器人Agent对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一； 一个交易员只能看到当前的交易价格； 一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。 在这种条件下：个体状态 ≠ 环境状态 正式地说，这种问题是一个部分可观测马尔可夫决策过程 (POMDP)。Agent 必须构建它自己的状态 representation $S^{a}_{t}$，比如： 记住完整的历史： $S^{a}_{t} = H_{t}$ 这种方法比较原始、幼稚。还有其他办法，例如 ： Beliefs of environment state：此时虽然 Agent 不知道环境状态到底是什么样，但Agent可以利用已有经验（数据），用各种 Agent 已知状态的概率分布作为当前时刻的 Agent 状态的呈现：$S^{a}_{t} = (P[S^e_t=s^1],…,P[S^e_t=s^n])$ Recurrent neural network：不需要知道概率，只根据当前的Agent状态以及当前时刻Agent的观测，送入循环神经网络(RNN)中得到一个当前Agent状态的呈现：$S^{a}_{t} = \\sigma(S^e_{t-1}W_s + O_tW_o)$ Agent的主要组成部分强化学习中的Agent可以由以下三个组成部分中的一个或多个组成： 1. 策略 Policy策略是决定Agent行为的机制。是从状态到行为的一个映射，可以是确定性的，也可以是不确定性的。 确定的 Policy : $a=\\pi(s)$ 随机 Policy : $\\pi(a|s) = P[A_t=a|S_t=s]$ 2. 价值函数 Value Function 是一个对未来奖励的预测 用来评价当前状态的好坏程度 当面对两个不同的状态时，Agent可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。同时，一个价值函数是基于某一个特定策略的，不同的策略下同一状态的价值并不相同。某一策略下的价值函数用下式表示：$V_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+…|S_t=s]$ 3. 模型 ModelAgent对环境的一个建模，它体现了Agent是如何思考环境运行机制的（how the agent think what the environment was.），Agent希望模型能模拟环境与Agent的交互机制。 模型至少要解决两个问题：一是状态转化概率，即预测下一个可能状态发生的概率：$P^a_{s s^{‘}} = P[S_{t+1}=s^{‘}|S_t=s,A_t=a]$ 另一项工作是预测可能获得的即时奖励：$R^a_{s} = E[R_{t+1}|S_t=s,A_t=a]$ 模型并不是构建一个Agent所必需的，很多强化学习算法中Agent并不试图（依赖）构建一个模型。 注：模型仅针对Agent而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定Agent下一个状态和所得的即时奖励。 Agent的分类解决强化学习问题，Agent可以有多种工具组合，比如通过建立对状态的价值的估计来解决问题，或者通过直接建立对策略的估计来解决问题。这些都是Agent可以使用的工具箱里的工具。因此，根据Agent内包含的“工具”进行分类，可以把Agent分为如下三类： 仅基于价值函数的 Value Based：在这样的Agent中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。 仅直接基于策略的 Policy Based：这样的Agent中行为直接由策略函数产生，Agent并不维护一个对各状态价值的估计函数。 演员-评判家形式 Actor-Critic：Agent既有价值函数、也有策略函数。两者相互结合解决问题。 此外，根据Agent在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类： Model Free:这类Agent并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。 Model Based：Agent尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。 学习和规划 Learning &amp; Planning学习： 环境初始时是未知的 Agent不知道环境如何工作 Agent通过与环境进行交互，逐渐改善其行为策略。 规划: 环境如何工作对于Agent是已知或近似已知的 Agent并不与环境发生实际的交互，而是利用其构建的模型进行计算 在此基础上改善其行为策略。 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。 探索和利用 Exploration &amp; Exploitation强化学习是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。 在这样的学习过程中，就会有一个在Exploration和Exploitation之间的权衡，前者是说会放弃一些已知的reward信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么action让reward比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让reward更大，即Exploration希望能够探索更多关于environment的信息。而后者是指根据已知的信息最大化reward。例如，在选择一个餐馆时，Exploitation会选择你最喜欢的餐馆，而Exploration会尝试选择一个新的餐馆。","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"}]},{"title":"从Fictitious Play 到 NFSP","date":"2017-07-27T13:13:00.000Z","path":"2017/07/27/从Fictitious Play 到 NFSP/","text":"博弈论Normal-form game在博弈论中，Normal-form game是对game的一种描述。Normal-form game通过矩阵来表示game。下图就是一个payoff矩阵：Normal-form game是一种静态模型，这个模型假设每个player仅选择一次action或策略。Normal-form game适用于描述不需要考虑博弈进程的完全信息静态(Complete information static)博弈。 静态博弈players同时进行决策 动态博弈players的决策顺序有先后 完全信息(Complete information)在完全信息博弈中，players的游戏结构和players的payoff矩阵是众所周知的，但players可能看不到其他players所做的所有动作。 完美信息(Perfect information)在完美信息博弈中，每个player可以观测到其他players的动作，但可能缺乏关于他人的payoff或游戏结构的信息。 Extensive-form gameExtensive-form game是一种动态模型。通过使用树这种图形来表示game。不仅给出了博弈结果，还描述了博弈过程，如给出博弈中参与人的行动顺序，以及决策环境和行动空间等。完美信息下的Extensive-form game:If player 1 plays D, player 2 will play U’ to maximise his payoff and so player 1 will only receive 1. However, if player 1 plays U, player 2 maximises his payoff by playing D’ and player 1 receives 2. Player 1 prefers 2 to 1 and so will play U and player 2 will play D’ 非完美信息下的Extensive-form game: 信息集 是一组决策节点： 集合中的每个节点都只属于一个player； 当player到达信息集时，player不能区分信息集中的节点。即，如果信息集包含多个节点，则该集合所属的player不知道集合中的哪个节点已经到达。 下图的虚线连接的两个节点2代表player2的一个信息集，player2不知道player1采取的是U还是D。补充：在扑克游戏中，player不知道的是对方的手牌信息而不是这个例子中的动作。player 2 cannot observe player 1’s move. Player 1 would like to fool player 2 into thinking he has played U when he has actually played D so that player 2 will play D’ and player 1 will receive 3. ref : https://en.wikipedia.org/wiki/Extensive-form_game Extensive-form game和Normal-form game的转换下图是Extensive-form game的一个例子：player1有三个信息集，每个信息集里面有两种行动，所以player1有8种策略；player2有两个信息集，每个信息集里面有两种行动，所以player2有4种策略。所以转换成Normal-form game是一个横8纵4的矩阵. FSP in Extensive-Form GamesExtensive-Form 定义扩充 ChanceChance被认为是一个特别的player，遵循一个固定的随机策略，决定了chance节点进入每个分支的概率分布。（如扑克游戏中的发牌）。 Information State我们假设游戏具有perfect recall，即每个player当前的信息状态u隐含了之前的信息状态和行动序列: Behavioural Strategy在给定Information State时，动作action的概率分布。Behavioural Strategy 输出player i在信息状态u下，动作的概率分布。Normal-Form 定义扩充 Pure Strategyplayer在可能遇到的所有情况下都有一个确定的动作action。(payoff矩阵中的每一行和列都是一个Pure Strategy) Mixed StrategyMixed Strateg是Player i的Pure Strategy的概率分布。 Fictitious Play在normal form下：当前策略 = 现有策略和bestResponse的加权平均。 Realization-equivalence也就是说，Extensive-Form下的Behavioural Strategy和Normal-form下的Mixed Strategy存在一种等价。Extensive-Form也可以使用Normal-form下的Fictitious Play来求纳什均衡解。 Extensive-Form Fictitious Play 代表到信息状态u的sequence = sequence 中每个节点选择每个action的概率乘积 下图是对Lemma 6的一个计算的验证。 我们通过Theorem7更新Behavioural Strategy，可以收敛到纳什均衡。 具体算法XFP： 计算bestResponse 更新策略，使用Theorem7 重复上面的过程 Fictitious Self-PlayXFP会有维度灾难。generalised weakened fictitious play只需要近似bestResponse，甚至允许更新中的一些扰动。 Fictitious Self-Play： 使用强化学习去计算bestResponse 使用监督学习更新策略 样本的采集 GENERATEDAT()方法需要在理解一下。 Neural Fictitious Self-Play强化学习和监督学习都使用神经网络去学习出一个函数。 FSP中样本采集的混合策略 ，在NSFP中是函数，不能简单相加。转换成 ，使用概率完成上面式子中的加操作。 REF： Topics in Algorithmic Game Theory February 8, 2010 Lecturer: Constantinos Daskalakis Lecture 2 Fictitious Self-Play in Extensive-Form Games Deep Reinforcement Learning from Self-play in imperfect-information games","tags":[{"name":"RL","slug":"RL","permalink":"https://gyh75520.github.io/tags/RL/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gyh75520.github.io/tags/博弈论/"},{"name":"ML","slug":"ML","permalink":"https://gyh75520.github.io/tags/ML/"}]},{"title":"opencv3 --python3 mac下配置","date":"2017-07-22T11:38:00.000Z","path":"2017/07/22/opencv3 --python3 mac配置/","text":"安装Anaconda把Anaconda安装的python版本设置为默认启动版本修改.bash_profile文件：123456789101112# Setting PATH for Python 2.7 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;&quot; export PATH # Setting PATH for Python 3.4 # The orginal version is saved in .bash_profile.pysave PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.4/bin:$&#123;PATH&#125;&quot; export PATH # added by howard export PATH=&quot;/usr/local/anaconda3/bin:$PATH&quot; PATH替换成自己的路径 根据该脚本，先会去找 /usr/local/anaconda3/bin ，发现有，就为当前路径下的解释器环境，并执行。—— 所以，想设置python的版本，直接把你想添加的路径export上去，并放在后面。ref: http://blog.csdn.net/u010692239/article/details/52701626 安装opencv1brew install opencv3 --with-python3 安装好后，在最后它会提示你如果想要Python也能调用opencv接口的话，需要执行下面命令：1234If you need Python to find bindings for this keg-only formula, run:```bashecho /usr/local/opt/opencv3/lib/python2.7/site-packages &gt;&gt; /usr/local/lib/python2.7/site-packages/opencv3.pth echo打印输出，&gt;&gt;重定向，执行完这句，可以在/usr/local/lib/python2.7/site-packages/目录下得到一个文件opencv3.pth。但是我们来看看它所放置的目录，这个目录是系统自带的Python目录，而我们使用的Anaconda里的Python，所以你需要将其重定向输出的路径改到Anaconda中Python目录下，比如我的：1echo /usr/local/opt/opencv3/lib/python3.6/site-packages &gt;&gt; /usr/local/anaconda3/lib/python3.6/site-packages/opencv3.pth ref: http://blog.csdn.net/willard_yuan/article/details/46721831 安装完成之后你一定会迫不及待的去测试一下：123pythonimport cv2RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa 如果出现上述错误，证明你numpy的版本不对，升级版本：1pip install numpy --upgrade","tags":[{"name":"opencv3","slug":"opencv3","permalink":"https://gyh75520.github.io/tags/opencv3/"},{"name":"mac","slug":"mac","permalink":"https://gyh75520.github.io/tags/mac/"}]},{"title":"HEXO+Github搭建博客","date":"2016-10-17T15:33:00.000Z","path":"2016/10/17/first/","text":"hexo是一款基于Node.js的静态博客框架。 之前是想着写博客，一方面是给自己做笔记，可以提升自己的写作、总结能力，这里记录一下linux下面Hexo搭建的步骤。 配置环境 安装Node.js 安装Git Github新建仓库和连接新建repository(仓库)登陆Github账号后，点击右上角的“+”号按钮，选择“New repository” 在Create a new repository界面填写仓库名必须为【your_user_name.github.io】固定写法 填写完成点Create repository创建完成 生成SSH Keys：我们如何让本地git项目与远程的github建立联系？这时候就要用到SSH Keys 使用ssh-keygen命令生成密钥对 1ssh-keygen -t rsa -C\"这里是你申请Github账号时的邮箱\" 然后系统会要你输入密码：（我们输入的密码会在你提交项目的时候使用） 12Enter passphrase (emptyforno passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; （终端提示生成的文件路径）找到你生成的密钥找到id_rsa.pub用终端进入编辑，复制密钥。 添加你的SSH Key到ssh-agent登陆Github,点击右侧用户按钮，选择Settings 点击 Add SSH key 按钮，将复制的密钥粘贴到 Key 栏 测试能不能链接成功1ssh -T git@github.com 执行结果 Permanently addedtheRSA host keyforIP address ‘192.30.252.130’tothelistofknown hosts.Are you sure you wanttocontinueconnecting (yes/no)?&lt;输入yes&gt;Hi username! You’ve successfully authenticated,butGitHubdoesnot 现在你已经可以通过SSH链接到Github了 正式安装HexoNode和Git都安装好后,首先创建一个文件夹,如blog,用户存放hexo的配置文件,然后进入blog里安装Hexo。我安装的是hexo 3.0以上的版本。 执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 初始化然后，执行init命令初始化hexo,命令： 1hexo init 好啦，至此，全部安装工作已经完成！blog就是你的博客根目录，所有的操作都在里面进行。 生成静态页面 1hexo generate（hexo g也可以） 本地启动 启动本地服务，进行文章预览调试，命令： 1hexo server 浏览器输入http://localhost:4000 然后建立关联，blog文件夹下有： _config.yml node_modules public source db.json package.json scaffolds themes 现在我们需要修改_config.yml文件，翻到最下面，改成像我这样：1234567deploy: type: git repo: https://github.com/**gyh75520**/**gyh75520**.github.io.git branch: master 加粗的部分替换成前面的配置的your_user_name 然后执行命令： 1npm install hexo-deployer-git --save 然后，执行配置命令： 1hexo deploy 到这里项目已经部署到了github pages上 访问https://your_user_name.github.io 查看自己的博客吧O(∩_∩)O 备注：没有权限的话记得在命令前加上sudo Mac篇在mac配置，执行如下命令安装Hexo： 1sudo npm install -g hexo-cli 报错:1npm ERR! 解决：把官方的源替换成淘宝的源，替换的方法12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install -g hexo-cli Hexo下mathjax的转义问题我们平时使用markdown写文档的时候，免不了会碰到数学公式，好在有强大的Mathjax,可以解析网页上的数学公式，与hexo的结合也很简单，可以手动加入js，或者直接使用hexo-math插件.大部分情况下都是可以的，但是Markdwon本身的特殊符号与Latex中的符号会出现冲突的时候: — 的转义，在markdown中，_是斜体，但是在latex中，却有下标的意思，就会出现问题。 \\\\\\的换行，在markdown中，\\\\\\会被转义为\\\\,这样也会影响影响mathjax对公式中的\\\\\\进行渲染 解决办法：参考 https://segmentfault.com/a/1190000007261752","tags":[{"name":"HEXO","slug":"HEXO","permalink":"https://gyh75520.github.io/tags/HEXO/"}]}]